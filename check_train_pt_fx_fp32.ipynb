{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa376bc-93a0-4537-88a3-882d82948b7e",
   "metadata": {},
   "source": [
    "### 1. Set the JAX platform (CPU/TPU) and matmul precision (if on TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f906ec-50c5-4bf4-9c56-e9353d753b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "os.environ[\"JAX_DEFAULT_MATMUL_PRECISION\"]=\"float32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee887206-b2d3-4888-acd2-cf7c36820aef",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42600b40-8ed3-425f-a4ce-2bb8d45b2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoConfig, AutoModelForSpeechSeq2Seq, FlaxAutoModelForSpeechSeq2Seq, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, FlaxSpeechEncoderDecoderModel, SpeechEncoderDecoderModel\n",
    "import flax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from numpy.random import default_rng\n",
    "import tempfile\n",
    "from flax.traverse_util import flatten_dict\n",
    "import torch\n",
    "from flax.training.common_utils import onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078854b-c20a-4a31-85db-25a315825cb1",
   "metadata": {},
   "source": [
    "### 3. Set model,training and data args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd4be40-8f1d-4da5-85ce-3cd1f82866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "encoder_id = \"hf-internal-testing/tiny-random-wav2vec2\"\n",
    "decoder_id = \"hf-internal-testing/tiny-random-bart\"\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-large-lv60\"\n",
    "decoder_id = \"facebook/bart-large\"\n",
    "\n",
    "# training args\n",
    "batch_size_per_update = 2\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# data args\n",
    "dataset_name = \"librispeech_asr\"\n",
    "dataset_config_name = \"clean\"\n",
    "train_split_name = \"train.100[:5%]\"\n",
    "eval_split_name = \"validation[:5%]\"\n",
    "dataset_cache_dir = \"/home/sanchitgandhi/cache/huggingface/datasets\"\n",
    "audio_column_name = \"audio\"\n",
    "text_column_name = \"text\"\n",
    "do_lower_case = True\n",
    "\n",
    "max_duration_in_seconds = 5\n",
    "min_duration_in_seconds = 0\n",
    "max_target_length = 32\n",
    "min_target_length = 0\n",
    "pad_input_to_multiple_of = 32000\n",
    "pad_target_to_multiple_of = None\n",
    "max_train_samples = max_eval_samples = None\n",
    "preprocessing_num_workers = num_workers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b603b3-0d21-4923-8eac-5faa9c0338a0",
   "metadata": {},
   "source": [
    "### 4. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2c1bc3-1393-4105-92b9-9b84a5bab4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=train_split_name,\n",
    "            cache_dir=dataset_cache_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505badb8-229e-498f-ab45-537e47e3119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets[\"eval\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=eval_split_name,\n",
    "            cache_dir=dataset_cache_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bfd05-c645-4871-b881-9c80ec47e83b",
   "metadata": {},
   "source": [
    "### 5. Load pretrained model, tokenizer, and feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61719803-6c0a-4001-b46e-96633e201fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing FlaxWav2Vec2Model: {('project_q', 'bias'), ('quantizer', 'weight_proj', 'bias'), ('project_hid', 'kernel'), ('quantizer', 'codevectors'), ('project_q', 'kernel'), ('quantizer', 'weight_proj', 'kernel'), ('project_hid', 'bias')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large were not used when initializing FlaxBartForCausalLM: {('encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'final_layer_norm', 'bias'), ('encoder', 'layers', '6', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '11', 'fc1', 'bias'), ('encoder', 'layers', '10', 'fc1', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('encoder', 'layers', '5', 'fc1', 'bias'), ('encoder', 'layers', '10', 'fc2', 'bias'), ('encoder', 'layers', '8', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'fc1', 'kernel'), ('encoder', 'layers', '2', 'fc1', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '2', 'fc1', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '10', 'final_layer_norm', 'bias'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('encoder', 'layers', '9', 'fc1', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '2', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '9', 'fc2', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'fc2', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '4', 'fc2', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '3', 'fc1', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '7', 'fc1', 'bias'), ('encoder', 'layers', '8', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '4', 'final_layer_norm', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '11', 'fc1', 'kernel'), ('encoder', 'embed_positions', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'fc1', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'fc2', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '10', 'fc2', 'kernel'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '5', 'fc2', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('encoder', 'layers', '5', 'fc2', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('encoder', 'layers', '11', 'final_layer_norm', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '5', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '9', 'fc2', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('encoder', 'layers', '3', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '3', 'fc1', 'kernel'), ('shared', 'kernel'), ('encoder', 'layernorm_embedding', 'bias'), ('encoder', 'layers', '7', 'fc1', 'kernel'), ('encoder', 'layers', '11', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '3', 'fc2', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '7', 'fc2', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '8', 'fc1', 'bias'), ('encoder', 'layers', '6', 'fc1', 'kernel'), ('encoder', 'layers', '3', 'final_layer_norm', 'bias'), ('encoder', 'layers', '8', 'fc1', 'kernel'), ('encoder', 'layers', '2', 'fc2', 'bias'), ('encoder', 'layers', '7', 'final_layer_norm', 'bias'), ('encoder', 'layers', '11', 'fc2', 'kernel'), ('encoder', 'layers', '2', 'fc2', 'kernel'), ('encoder', 'layers', '4', 'fc1', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'fc2', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('encoder', 'layers', '11', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '6', 'final_layer_norm', 'bias'), ('encoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '5', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '7', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'fc1', 'bias'), ('encoder', 'layernorm_embedding', 'kernel'), ('encoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'fc2', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '7', 'fc2', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('encoder', 'layers', '3', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'fc1', 'bias'), ('encoder', 'layers', '7', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '4', 'fc1', 'kernel'), ('encoder', 'layers', '8', 'fc2', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '6', 'fc2', 'kernel'), ('encoder', 'layers', '8', 'fc2', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-05 09:30:55.541052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing Wav2Vec2Model: ['quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large were not used when initializing BartForCausalLM: ['encoder.layers.5.fc1.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.9.fc1.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.8.fc1.bias', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.9.fc2.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.6.fc1.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.7.fc2.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.9.fc2.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.6.fc1.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.8.fc1.weight', 'shared.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.7.fc1.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.11.fc2.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.11.fc1.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.embed_tokens.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.10.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.11.fc1.bias', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.7.fc1.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.8.fc2.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.10.fc1.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "fx_model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_from_pt=True, decoder_from_pt=True)\n",
    "\n",
    "fx_model.config.decoder_start_token_id = fx_model.config.decoder.bos_token_id\n",
    "fx_model.config.pad_token_id = fx_model.config.decoder.pad_token_id\n",
    "fx_model.config.eos_token_id = fx_model.config.decoder.eos_token_id\n",
    "fx_model.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "pt_model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
    "pt_model.config.decoder_start_token_id = pt_model.config.decoder.bos_token_id\n",
    "pt_model.config.pad_token_id = pt_model.config.decoder.pad_token_id\n",
    "pt_model.config.eos_token_id = pt_model.config.decoder.eos_token_id\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "processor = AutoProcessor.from_pretrained(encoder_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "\n",
    "if fx_model.config.decoder_start_token_id or pt_model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cd33a-7446-47f5-8864-799165f3bec3",
   "metadata": {},
   "source": [
    "### 6. Check that the PT and FX weights are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02e0d09-6d9e-4b25-985d-a4dc4ee87a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /tmp/tmp61gywpfv were not used when initializing FlaxSpeechEncoderDecoderModel: {('decoder', 'lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Convert the PT model to FX to enable comparison of param dicts (PT state dict -> FX param dict)\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_model.save_pretrained(tmpdirname)\n",
    "    pt_model_to_fx = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(tmpdirname, from_pt=True)\n",
    "    \n",
    "# It's easier to view the PyTree param dict when flattened\n",
    "fx_params = flatten_dict(fx_model.params)\n",
    "pt_params_to_fx = flatten_dict(pt_model_to_fx.params)\n",
    "\n",
    "# Check that the keys match\n",
    "assert fx_params.keys() == pt_params_to_fx.keys()\n",
    "\n",
    "# Check that the parameters are precisely equal\n",
    "for param in fx_params:\n",
    "    assert (fx_params[param] == pt_params_to_fx[param]).all(), f\"{param} weights are not equal between Flax and PyTorch\"\n",
    "    \n",
    "# Free CPU memory\n",
    "del fx_params, pt_params_to_fx, pt_model_to_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab52e9c-fc1a-475d-a79b-3fdac410a053",
   "metadata": {},
   "source": [
    "### 7. Resample speech dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35edc92d-7124-4fea-b658-5ecfba1f4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Torch audio in this resampling step for convinience\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[audio_column_name].sampling_rate\n",
    "if dataset_sampling_rate != feature_extractor.sampling_rate:\n",
    "    raw_datasets = raw_datasets.cast_column(\n",
    "        audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e50f5-b37e-42e7-b34c-b29614686c69",
   "metadata": {},
   "source": [
    "### 8. Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460a0da7-fb93-43e5-8d86-7797ac6839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "max_input_length = int(max_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "min_input_length = int(min_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49af1f99-a8e7-4615-9779-dd176cd5bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate data to max_samples\n",
    "if max_train_samples is not None:\n",
    "        raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(max_train_samples))\n",
    "\n",
    "if max_eval_samples is not None:\n",
    "    raw_datasets[\"eval\"] = raw_datasets[\"eval\"].select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a8563a-70b2-45cb-91b6-69c9d78c6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    batch[\"labels_length\"] = len(batch[\"labels\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df4d09b-b6a8-4cdc-83d9-accb2108ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-db5d2c9f36ccc8cd.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-2c4e7cfc87703302.arrow\n"
     ]
    }
   ],
   "source": [
    "vectorized_datasets = raw_datasets.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=next(iter(raw_datasets.values())).column_names,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            desc=\"preprocess train dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0dcd3f-2e3f-4f65-8111-b5682a7b0f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-dc1cbfa3e7304ac2.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-228e1ef514f5c637.arrow\n"
     ]
    }
   ],
   "source": [
    "# filter data with inputs shorter than min_input_length or longer than max_input_length\n",
    "def is_audio_in_length_range(length):\n",
    "    return length > min_input_length and length < max_input_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1acf9a7f-2dad-4131-9c42-b6361ace1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-1f73bf8419e71173.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-5940edcb91dd459a.arrow\n"
     ]
    }
   ],
   "source": [
    "# filter data with targets shorter than min_target_length or longer than max_target_length\n",
    "def is_labels_in_length_range(length):\n",
    "    return length > min_target_length and length < max_target_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_labels_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"labels_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4113029-dd51-4d4c-8f71-74b52c42689f",
   "metadata": {},
   "source": [
    "### 9. Define DataCollators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325456af-0560-4c13-bbff-8b67ba5407d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch DataCollator\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec65219-9e94-42d9-9018-5bb57bb04549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax DataCollator\n",
    "@flax.struct.dataclass\n",
    "class FlaxDataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    input_padding: Union[bool, str] = \"longest\"\n",
    "    target_padding: Union[bool, str] = \"max_length\"\n",
    "    max_input_length: Optional[float] = None\n",
    "    max_target_length: Optional[int] = None\n",
    "    pad_input_to_multiple_of: Optional[int] = None\n",
    "    pad_target_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=self.input_padding,\n",
    "            pad_to_multiple_of=self.pad_input_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=self.target_padding,\n",
    "            pad_to_multiple_of=self.pad_target_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]\n",
    "\n",
    "        decoder_input_ids = shift_tokens_right(labels, self.decoder_start_token_id)\n",
    "\n",
    "        # replace padding with -100 to ignore correctly when computing the loss\n",
    "        labels = np.ma.array(labels, mask=np.not_equal(labels_batch.attention_mask, 1))\n",
    "        labels = labels.filled(fill_value=-100)\n",
    "\n",
    "        batch[\"inputs\"] = batch.pop(\"input_values\")\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return batch\n",
    "    \n",
    "def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift label ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_label_ids = np.zeros_like(label_ids)\n",
    "    shifted_label_ids[:, 1:] = label_ids[:, :-1]\n",
    "    shifted_label_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    return shifted_label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb6699-2e55-459d-8b68-4c8f46c16874",
   "metadata": {},
   "source": [
    "### 10. Define length grouped sampler (PT and FX compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcacdc57-d168-4be5-ba2a-738031aa4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_indices(\n",
    "    dataset, batch_size: int, rng: Optional[List[int]] = None, mega_batch_mult: Optional[int] = None\n",
    ") -> np.array:\n",
    "    lengths = dataset[\"input_length\"]\n",
    "\n",
    "    # Default for mega_batch_mult: 50 or the number to get 4 megabatches, whichever is smaller.\n",
    "    if mega_batch_mult is None:\n",
    "        mega_batch_mult = min(len(lengths) // (batch_size * 4), 50)\n",
    "        # Just in case, for tiny datasets\n",
    "        if mega_batch_mult == 0:\n",
    "            mega_batch_mult = 1\n",
    "\n",
    "    # We need to use JAX for the random permutation as the PRNG key will be set based on the seed outside of the sampler.\n",
    "    num_samples = len(lengths)\n",
    "    indices = jax.random.permutation(rng, np.arange(num_samples)) if rng is not None else np.arange(num_samples)\n",
    "\n",
    "    megabatch_size = mega_batch_mult * batch_size\n",
    "    megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]\n",
    "    megabatches = [list(sorted(megabatch, key=lambda i: lengths[i], reverse=True)) for megabatch in megabatches]\n",
    "\n",
    "    # The rest is to get the biggest batch first.\n",
    "    # Since each megabatch is sorted by descending length, the longest element is the first\n",
    "    megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]\n",
    "    max_idx = np.argmax(megabatch_maximums).item()\n",
    "    # Switch to put the longest batch in first position\n",
    "    # (note that this is different to the PT grouped sampler in which we only put the longest element in the first position, and not its batch)\n",
    "    megabatches[0], megabatches[max_idx] = megabatches[max_idx], megabatches[0]\n",
    "\n",
    "    megabatches = np.array([i for megabatch in megabatches for i in megabatch])\n",
    "\n",
    "    return megabatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20628c0-cca6-4bd2-b153-bb38e0a6a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group samples into batch splits\n",
    "def generate_batch_splits(samples_idx: jnp.ndarray, batch_size: int) -> jnp.ndarray:\n",
    "    num_samples = len(samples_idx)\n",
    "    samples_to_remove = num_samples % batch_size\n",
    "\n",
    "    if samples_to_remove != 0:\n",
    "        samples_idx = samples_idx[:-samples_to_remove]\n",
    "    sections_split = num_samples // batch_size\n",
    "    batch_idx = np.split(samples_idx, sections_split)\n",
    "    return batch_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c5fd4-2d9b-4627-9624-9044a940fbc8",
   "metadata": {},
   "source": [
    "### 11. Helper funcitons for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b19eae5e-c72d-420c-9ea0-80e0fb786d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_almost_equals(a: np.ndarray, b: np.ndarray, tol: float = 1e-2):\n",
    "    \"\"\"Assert whether the maximum absolute difference between two NumPy arrays a and b is within a given tolerance tol. \n",
    "    Due to the pad_to_multiple_of nature of the FlaxDataCollator, the length of the Flax array a will always be greater than \n",
    "    or equal to the length of the PyTorch array b. If a and b are of different lengths, array a (Flax, padded) will be \n",
    "    reshaped to the shape of b (PyTorch).\"\"\"\n",
    "    if a.shape != b.shape:\n",
    "        a = a[:, :b.shape[1]]\n",
    "    \n",
    "    diff = np.abs((a - b))\n",
    "    if diff.max() < tol:\n",
    "        print(f\"✅ Difference between Flax and PyTorch is {diff.max()} (< {tol}), avg is {diff.mean()}\")\n",
    "    else:\n",
    "        print(f\"❌ Difference between Flax and PyTorch is {diff.max()} (>= {tol}), avg is {diff.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189b0366-34dc-409f-95e3-8010842543bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dict_equal(a: dict, b: dict, tol: float = 1e-2):\n",
    "    if a.keys() != b.keys():\n",
    "        print(\"❌ Dictionary keys for PyTorch and Flax do not match\")\n",
    "    results_fail = []\n",
    "    results_correct = []\n",
    "\n",
    "    results_fail_rel = []\n",
    "    results_correct_rel = []\n",
    "    for k in a:\n",
    "        ak_norm = np.linalg.norm(a[k])\n",
    "        bk_norm = np.linalg.norm(b[k])\n",
    "        diff = np.abs(ak_norm - bk_norm)\n",
    "        diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n",
    "        if diff < tol:\n",
    "            results_correct.append(f\"✅ Layer {k} diff is {diff} < {tol}).\")\n",
    "        else:\n",
    "            results_fail.append(f\"❌ Layer {k} has PT grad norm {bk_norm} and flax grad norm {ak_norm}.\")\n",
    "        if diff_rel < tol:\n",
    "            results_correct_rel.append(f\"✅ Layer {k} rel diff is {diff} < {tol}).\")\n",
    "        else:\n",
    "            results_fail_rel.append(f\"❌ Layer {k} has PT grad norm {bk_norm} and flax grad norm {ak_norm}.\")\n",
    "    return results_fail_rel, results_correct_rel, results_fail, results_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "869586c7-1bf5-4c8a-a80c-c920376007bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def kl_divergence(a: np.ndarray, b:np.ndarray, epsilon=1e-6, tol: float = 1e-2):\n",
    "    \"\"\"Epsilon is used here to avoid conditional code for checking that neither p(a) nor p(b) is equal to 0.\"\"\"\n",
    "    if a.shape[1] != b.shape[1]:\n",
    "        a = a[:, :b.shape[1], :]\n",
    "        \n",
    "    p_a = softmax(a) + epsilon\n",
    "    p_b = softmax(b) + epsilon\n",
    "    divergence = np.sum(p_b * np.log(p_b / p_a))\n",
    "    if divergence < tol:\n",
    "        print(f\"✅ KL divergence between Flax and PyTorch is {divergence} (< {tol})\")\n",
    "    else:\n",
    "        print(f\"❌ KL divergence between Flax and PyTorch is {divergence} (>= {tol})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9a12e-08a5-4d3d-af93-5575847d7e3e",
   "metadata": {},
   "source": [
    "### 12. Instantiate data collators and generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53cc0adc-e7d1-4b94-ae56-bd29b1908994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PT and FX DataCollators\n",
    "pt_data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=pt_model.config.decoder_start_token_id,\n",
    "    )\n",
    "\n",
    "fx_data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=fx_model.config.decoder_start_token_id,\n",
    "        input_padding=\"longest\",\n",
    "        target_padding=\"max_length\",\n",
    "        max_target_length=max_target_length,\n",
    "        pad_input_to_multiple_of=pad_input_to_multiple_of,\n",
    "        pad_target_to_multiple_of=pad_target_to_multiple_of,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c87eb0-e120-46df-ba9d-31f0862fb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAX seed and generate PRNG for stochasic operations\n",
    "seed = 0\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, input_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2383df6f-e2e2-481e-8f7d-ec75b92baaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll naively create our batches through random shuffling and no grouping by length\n",
    "num_train_samples = len(vectorized_datasets[\"train\"])\n",
    "train_samples_idx = jax.random.permutation(input_rng, np.arange(num_train_samples))\n",
    "train_batch_idx = generate_batch_splits(train_samples_idx, batch_size_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b67470de-1374-410a-9144-6c2b5ff58874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt: we'll use the grouped sampler\n",
    "train_samples_idx = get_grouped_indices(vectorized_datasets[\"train\"], batch_size_per_update, input_rng)\n",
    "train_batch_idx = generate_batch_splits(train_samples_idx, batch_size_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd00ffbf-b613-46cf-a7ba-03597f99ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the first training batch\n",
    "batch_idx = train_batch_idx[1]\n",
    "samples = [vectorized_datasets[\"train\"][int(idx)] for idx in batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85377ea3-3616-4ba4-8f14-52efe488f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_batch = fx_data_collator(samples)\n",
    "pt_batch = pt_data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0be611-5b08-43cb-a10a-5adb05c6297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Flax inputs to PyTorch (optional)\n",
    "#pt_batch = {k: torch.tensor(v.tolist()) for k, v in fx_batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949c45a-75e7-40a2-b55e-efa60c476517",
   "metadata": {},
   "source": [
    "### 13. Check that the inputs are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c47d8814-dc72-4c5b-b5d2-bf2c2f2af3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['attention_mask', 'inputs', 'labels', 'decoder_input_ids']),\n",
       " dict_keys(['input_values', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_fx_keys = [\"inputs\", \"labels\", \"decoder_input_ids\"]\n",
    "expected_pt_keys = [\"input_values\", \"labels\"]\n",
    "\n",
    "for expected_fx_key in expected_fx_keys:\n",
    "    assert expected_fx_key in fx_batch, f\"{expected_fx_key} not in Flax batched inputs\"\n",
    "\n",
    "for expected_pt_key in expected_pt_keys:\n",
    "    assert expected_pt_key in pt_batch, f\"{expected_pt_key} not in PyTorch batched inputs\"    \n",
    "\n",
    "# Expect the keys between Flax and PyTorch to be different, this is just for observation\n",
    "fx_batch.keys(), pt_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9acf4d02-e5e7-4452-b3c0-8e6fb4873a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01), avg is 0.0\n",
      "✅ Difference between Flax and PyTorch is 0 (< 0.01), avg is 0.0\n",
      "✅ Difference between Flax and PyTorch is 0 (< 0.01), avg is 0.0\n"
     ]
    }
   ],
   "source": [
    "assert_almost_equals(fx_batch['inputs'], pt_batch['input_values'].numpy())\n",
    "assert_almost_equals(fx_batch['labels'], pt_batch['labels'].numpy())\n",
    "if 'attention_mask' in fx_batch.keys() and pt_batch.keys():\n",
    "    assert_almost_equals(fx_batch['attention_mask'], pt_batch['attention_mask'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a599272-1f9a-4711-bc1b-908ae0b6006b",
   "metadata": {},
   "source": [
    "### 14. Run a training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c33f27f5-8387-41b2-9b09-91b15b1423da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch, output_hidden_states=True)\n",
    "pt_logits = pt_outputs.logits\n",
    "pt_loss = pt_outputs.loss\n",
    "pt_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9843812b-d3f2-4f3b-bcaa-5176ced950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax cross entropy loss\n",
    "def loss_fn(logits, labels):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = optax.softmax_cross_entropy(logits, onehot(labels, vocab_size))\n",
    "    # ignore padded tokens from loss, i.e. where labels are not set to -100\n",
    "    padding = labels >= 0\n",
    "    loss = loss * padding\n",
    "    loss = loss.sum() / padding.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5db6a35b-d2cc-42d4-98b1-cbaac62d7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax training step (single device)\n",
    "def fx_train_step(fx_model, fx_batch, output_hidden_states=True):\n",
    "    def compute_loss(params):\n",
    "        labels = fx_batch.pop(\"labels\")\n",
    "        outputs = fx_model(**fx_batch, params=params, output_hidden_states=output_hidden_states)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        return loss, outputs\n",
    "\n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, outputs), grad = grad_fn(fx_model.params)\n",
    "    \n",
    "    return loss, outputs, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd82d9ef-2dc1-41dc-9855-45ccfdd02413",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_loss, fx_outputs, fx_grad = fx_train_step(fx_model, fx_batch, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a30c5b-ca1c-44a1-9fb1-255e908c8e5b",
   "metadata": {},
   "source": [
    "### 15. Compare outputs for the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6674278-d69f-4ab3-9bf2-f80706b9af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder hidden states match--------------------------\n",
      "✅ Difference between Flax and PyTorch is 0.00087738037109375 (< 0.01), avg is 2.060942460957449e-05\n",
      "✅ Difference between Flax and PyTorch is 0.0009002685546875 (< 0.01), avg is 3.303395351395011e-05\n",
      "✅ Difference between Flax and PyTorch is 0.000911712646484375 (< 0.01), avg is 3.521700273267925e-05\n",
      "✅ Difference between Flax and PyTorch is 0.000858306884765625 (< 0.01), avg is 3.61947895726189e-05\n",
      "✅ Difference between Flax and PyTorch is 0.0008144378662109375 (< 0.01), avg is 3.6649136745836586e-05\n",
      "✅ Difference between Flax and PyTorch is 0.00079345703125 (< 0.01), avg is 3.6703062505694106e-05\n",
      "✅ Difference between Flax and PyTorch is 0.0012302398681640625 (< 0.01), avg is 3.7468114896910265e-05\n",
      "❌ Difference between Flax and PyTorch is 0.0205078125 (>= 0.01), avg is 3.865711187245324e-05\n",
      "❌ Difference between Flax and PyTorch is 0.021484375 (>= 0.01), avg is 3.895111512974836e-05\n",
      "❌ Difference between Flax and PyTorch is 0.021484375 (>= 0.01), avg is 4.3906489736400545e-05\n",
      "❌ Difference between Flax and PyTorch is 0.0220947265625 (>= 0.01), avg is 7.263526640599594e-05\n",
      "❌ Difference between Flax and PyTorch is 0.0228271484375 (>= 0.01), avg is 0.0001445348170818761\n",
      "❌ Difference between Flax and PyTorch is 0.02294921875 (>= 0.01), avg is 0.00019694205548148602\n",
      "❌ Difference between Flax and PyTorch is 0.021728515625 (>= 0.01), avg is 0.0002771755389403552\n",
      "❌ Difference between Flax and PyTorch is 0.0208740234375 (>= 0.01), avg is 0.0003209611459169537\n",
      "❌ Difference between Flax and PyTorch is 0.019775390625 (>= 0.01), avg is 0.00034452383988536894\n",
      "❌ Difference between Flax and PyTorch is 0.019287109375 (>= 0.01), avg is 0.00039492029463872313\n",
      "❌ Difference between Flax and PyTorch is 0.0189208984375 (>= 0.01), avg is 0.00043256793287582695\n",
      "❌ Difference between Flax and PyTorch is 0.0194091796875 (>= 0.01), avg is 0.0004682737053371966\n",
      "❌ Difference between Flax and PyTorch is 0.0191650390625 (>= 0.01), avg is 0.000519916124176234\n",
      "❌ Difference between Flax and PyTorch is 0.0185546875 (>= 0.01), avg is 0.0005588934291154146\n",
      "❌ Difference between Flax and PyTorch is 0.01751708984375 (>= 0.01), avg is 0.0005939818802289665\n",
      "❌ Difference between Flax and PyTorch is 0.802734375 (>= 0.01), avg is 0.004085644148290157\n",
      "❌ Difference between Flax and PyTorch is 1.4990234375 (>= 0.01), avg is 0.004691453650593758\n",
      "✅ Difference between Flax and PyTorch is 0.00014787912368774414 (< 0.01), avg is 2.1688806839392782e-07\n",
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "✅ Difference between Flax and PyTorch is 0.00014787912368774414 (< 0.01), avg is 2.1688806839392782e-07\n",
      "--------------------------Checking decoder hidden states match--------------------------\n",
      "❌ Difference between Flax and PyTorch is 3.37753963470459 (>= 0.01), avg is 0.023428764194250107\n",
      "❌ Difference between Flax and PyTorch is 2.4688453674316406 (>= 0.01), avg is 0.017305472865700722\n",
      "❌ Difference between Flax and PyTorch is 2.245556354522705 (>= 0.01), avg is 0.010741560719907284\n",
      "❌ Difference between Flax and PyTorch is 1.310788631439209 (>= 0.01), avg is 0.005404260940849781\n",
      "❌ Difference between Flax and PyTorch is 0.1572081744670868 (>= 0.01), avg is 0.0005812643794342875\n",
      "✅ Difference between Flax and PyTorch is 0.009916186332702637 (< 0.01), avg is 0.00011381421791156754\n",
      "✅ Difference between Flax and PyTorch is 0.0008476302027702332 (< 0.01), avg is 8.093680662568659e-06\n",
      "✅ Difference between Flax and PyTorch is 5.486258305609226e-05 (< 0.01), avg is 8.353307521247189e-07\n",
      "✅ Difference between Flax and PyTorch is 5.941838026046753e-06 (< 0.01), avg is 2.3075169508501858e-07\n",
      "✅ Difference between Flax and PyTorch is 9.98377799987793e-07 (< 0.01), avg is 4.7071623043848376e-08\n",
      "✅ Difference between Flax and PyTorch is 1.5795230865478516e-06 (< 0.01), avg is 6.224288284784052e-08\n",
      "✅ Difference between Flax and PyTorch is 1.7583370208740234e-06 (< 0.01), avg is 1.574136376802926e-07\n",
      "✅ Difference between Flax and PyTorch is 5.031749606132507e-05 (< 0.01), avg is 6.587420557480073e-06\n",
      "--------------------------Checking logits match--------------------------\n",
      "Flax logits shape: (2, 31, 50265), PyTorch logits shape: torch.Size([2, 21, 50265])\n",
      "✅ Difference between Flax and PyTorch is 0.00018787384033203125 (< 0.01), avg is 2.024477180384565e-05\n",
      "✅ KL divergence between Flax and PyTorch is 0.00028929300606250763 (< 0.01)\n",
      "--------------------------Checking losses match--------------------------\n",
      "Flax loss: 25.756391525268555, PyTorch loss: 25.756454467773438\n",
      "✅ Difference between Flax and PyTorch is 6.29425048828125e-05 (< 0.01), avg is 6.29425048828125e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder hidden states match--------------------------\")\n",
    "for fx_state, pt_state in zip(fx_outputs.encoder_hidden_states, pt_outputs.encoder_hidden_states):\n",
    "    assert_almost_equals(fx_state, pt_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "assert_almost_equals(fx_outputs.encoder_last_hidden_state, pt_outputs.encoder_last_hidden_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking decoder hidden states match--------------------------\")\n",
    "for fx_state, pt_state in zip(fx_outputs.decoder_hidden_states, pt_outputs.decoder_hidden_states):\n",
    "    assert_almost_equals(fx_state, pt_state.detach().numpy())\n",
    "    \n",
    "print(\"--------------------------Checking logits match--------------------------\")\n",
    "print(f\"Flax logits shape: {fx_outputs.logits.shape}, PyTorch logits shape: {pt_logits.shape}\")\n",
    "assert_almost_equals(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "kl_divergence(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking losses match--------------------------\")\n",
    "print(f\"Flax loss: {fx_loss}, PyTorch loss: {pt_loss}\")\n",
    "assert_almost_equals(fx_loss, pt_loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8665ed-8f9b-44c3-bb27-e0c4f233726e",
   "metadata": {},
   "source": [
    "### 16. Compare outputs for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "128835da-5a10-4dc3-bb3a-ff530be70651",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_grad_dict = {k: v.grad if v.grad is not None else torch.zeros(v.shape) for k, v in pt_model.named_parameters()}\n",
    "missing_grads = [k for k in pt_model.state_dict().keys() if k not in pt_grad_dict]\n",
    "\n",
    "missing_keys, unexpected_keys = pt_model.load_state_dict(pt_grad_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7298d88f-8d06-4f6c-be85-ae7c4ffb8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert missing_grads == missing_keys, f\"Error with either grads {missing_keys} or keys {unexpected_keys}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c52fe36-304f-41b6-ac45-b250183e74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /tmp/tmpttac__gx were not used when initializing FlaxSpeechEncoderDecoderModel: {('decoder', 'lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_model.save_pretrained(tmpdirname)\n",
    "    pt_grad_model_to_fx = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(tmpdirname, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd037b4a-609d-44b6-9e4a-9de0525f6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_grad_to_fx = pt_grad_model_to_fx.params\n",
    "fx_grad = flatten_dict(fx_grad)\n",
    "pt_grad_to_fx = flatten_dict(pt_grad_to_fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6022185e-71de-4338-8150-ffb819bce349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1311785/2310070863.py:13: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n",
      "/tmp/ipykernel_1311785/2310070863.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n"
     ]
    }
   ],
   "source": [
    "results_fail_rel, results_correct_rel, results_fail, results_correct = assert_dict_equal(fx_grad, pt_grad_to_fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16f0517d-5bff-4825-8a30-5fa86723f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking gradients match--------------------------\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'embed_tokens', 'embedding') has PT grad norm 39.86210250854492 and flax grad norm 39.875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 83.90470123291016 and flax grad norm 83.875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 58.8936653137207 and flax grad norm 58.875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc1', 'kernel') has PT grad norm 552.0279541015625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc2', 'kernel') has PT grad norm 93.37858581542969 and flax grad norm 93.0625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'final_layer_norm', 'scale') has PT grad norm 144.46214294433594 and flax grad norm 144.5.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'bias') has PT grad norm 43.27117156982422 and flax grad norm 43.28125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel') has PT grad norm 53.61735534667969 and flax grad norm 53.5625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel') has PT grad norm 119.48453521728516 and flax grad norm 119.3125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 279.05181884765625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 162.196044921875 and flax grad norm 162.125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn_layer_norm', 'bias') has PT grad norm 45.44382858276367 and flax grad norm 45.46875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc1', 'kernel') has PT grad norm 305.48760986328125 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc2', 'kernel') has PT grad norm 254.1536865234375 and flax grad norm 253.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'bias') has PT grad norm 97.77052307128906 and flax grad norm 97.75.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel') has PT grad norm 169.980224609375 and flax grad norm 169.875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'bias') has PT grad norm 99.77791595458984 and flax grad norm 99.75.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel') has PT grad norm 175.14663696289062 and flax grad norm 175.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'fc1', 'kernel') has PT grad norm 10.76535415649414 and flax grad norm 10.75.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'fc2', 'kernel') has PT grad norm 8.510902404785156 and flax grad norm 8.4765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 34.45231628417969 and flax grad norm 34.4375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc1', 'kernel') has PT grad norm 285.70281982421875 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc2', 'kernel') has PT grad norm 51.36979293823242 and flax grad norm 51.25.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'final_layer_norm', 'bias') has PT grad norm 65.48123168945312 and flax grad norm 65.5.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel') has PT grad norm 49.815711975097656 and flax grad norm 49.75.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 13.33632755279541 and flax grad norm 13.325236320495605.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'conv', 'bias') has PT grad norm 53.999027252197266 and flax grad norm 53.95066452026367.\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking gradients match--------------------------\")\n",
    "if len(results_fail) == 0:\n",
    "    print(\"✅ All grads pass\")\n",
    "else:\n",
    "    print(\"\\n\".join(results_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ab13c17-7151-4f10-a9fa-0296914678e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking rel gradients match--------------------------\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'embed_positions', 'embedding') has PT grad norm 0.0009617977775633335 and flax grad norm 0.0009765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layernorm_embedding', 'scale') has PT grad norm 0.0005227156216278672 and flax grad norm 0.0005459785461425781.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 3.740085324777098e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 5.6070362916216254e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0001576704962644726 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 2.3031611817714293e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 3.5919500078307465e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.000444757315563038 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.00048297044122591615 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0004706356849055737 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'fc1', 'bias') has PT grad norm 0.0002640218590386212 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'fc2', 'bias') has PT grad norm 0.0003810059861280024 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias') has PT grad norm 2.8044976063679172e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.00028946585371159017 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias') has PT grad norm 0.00010499362542759627 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0008934183861128986 and flax grad norm 0.0008802413940429688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.0004153798508923501 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00015797442756593227 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00017602449224796146 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 3.185018221385505e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 0.00014299941540230066 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0002705826482269913 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 4.7828330025367904e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 0.00010528130951570347 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0006108622183091938 and flax grad norm 0.0005979537963867188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.0008279162575490773 and flax grad norm 0.0008096694946289062.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'fc1', 'bias') has PT grad norm 0.0005070572951808572 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'fc2', 'bias') has PT grad norm 0.0007143656839616597 and flax grad norm 0.000732421875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias') has PT grad norm 6.921692385919442e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel') has PT grad norm 0.0006103404448367655 and flax grad norm 0.0005979537963867188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias') has PT grad norm 6.314019265118986e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0006786587182432413 and flax grad norm 0.000690460205078125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.0008599572465755045 and flax grad norm 0.0008459091186523438.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'bias') has PT grad norm 0.0002709675463847816 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00039250036934390664 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 1.6359459209525085e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc1', 'kernel') has PT grad norm 552.0279541015625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.0165590680344394e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel') has PT grad norm 2.52727858196522e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'bias') has PT grad norm 1.5384832963150075e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel') has PT grad norm 3.82086788874858e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 2.5305797635155614e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 279.05181884765625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc1', 'kernel') has PT grad norm 305.48760986328125 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'bias') has PT grad norm 2.0574725922983816e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel') has PT grad norm 3.589737218590017e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'bias') has PT grad norm 5.373528466634525e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel') has PT grad norm 9.379733256764666e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 4.506685302718694e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 0.0003341579285915941 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0006355933146551251 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 9.996992048399989e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 0.00019818365399260074 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'fc1', 'bias') has PT grad norm 0.0008665498462505639 and flax grad norm 0.0008802413940429688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'fc2', 'bias') has PT grad norm 0.0006541278562508523 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.6330914051621015e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.0010253244545310736 and flax grad norm 0.0010356903076171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'bias') has PT grad norm 5.106745447847061e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0006065217894501984 and flax grad norm 0.0005979537963867188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'bias') has PT grad norm 0.0006358700920827687 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 1.504891697368116e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 5.252928895060904e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0003855098912026733 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 1.4055840438231826e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 3.4981272619916126e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0005586237530224025 and flax grad norm 0.0005459785461425781.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'fc1', 'bias') has PT grad norm 0.00017045566346496344 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'fc2', 'bias') has PT grad norm 0.00014011967869009823 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.1046415518412012e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'bias') has PT grad norm 7.391737017314881e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0008550950442440808 and flax grad norm 0.0008459091186523438.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00038519641384482384 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'scale') has PT grad norm 0.0004377489385660738 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 4.6388944768827045e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 9.567788220010698e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.00015257026825565845 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 2.7510216114023933e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 6.205538375070319e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0003639751812443137 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.00041411464917473495 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0004386305226944387 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'fc1', 'bias') has PT grad norm 0.0002191578969359398 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'fc2', 'bias') has PT grad norm 0.00010923683294095099 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'bias') has PT grad norm 4.557266564987617e-13 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel') has PT grad norm 8.098435500869527e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'bias') has PT grad norm 2.413846459603519e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel') has PT grad norm 5.345232693798607e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00015249331772793084 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00016941216017585248 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 6.942013630606425e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0004428474640008062 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 5.706692536477931e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'fc2', 'bias') has PT grad norm 0.0002488051832187921 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.4497721599615598e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel') has PT grad norm 4.0030977288552094e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'bias') has PT grad norm 1.3804740319756093e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel') has PT grad norm 2.2529673060489586e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00047144826385192573 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'scale') has PT grad norm 0.0004624506982509047 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 1.698367002589407e-10 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 3.2589152397122234e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.3996489514264687e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel') has PT grad norm 2.4567978584855155e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'bias') has PT grad norm 6.858999057612891e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel') has PT grad norm 8.986243216213552e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 2.742335469818613e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 0.0005188906216062605 and flax grad norm 0.0005459785461425781.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'bias') has PT grad norm 5.571154648720267e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel') has PT grad norm 2.5207384624081897e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'bias') has PT grad norm 7.539117774513215e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel') has PT grad norm 1.249353775989448e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 5.697579297248012e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'bias') has PT grad norm 7.060892842636335e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel') has PT grad norm 4.777335416150663e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'bias') has PT grad norm 7.13960390807955e-10 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel') has PT grad norm 2.634178652982655e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 8.63976197251759e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc1', 'kernel') has PT grad norm 285.70281982421875 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.825268713950834e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel') has PT grad norm 8.711315224729788e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'bias') has PT grad norm 1.669988258790056e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel') has PT grad norm 7.908984578364198e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'k_proj', 'bias') has PT grad norm 4.651916896136754e-08 and flax grad norm 1.2264301574305136e-07.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'k_proj', 'bias') has PT grad norm 5.876750464040015e-08 and flax grad norm 1.0658874316504807e-07.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'k_proj', 'bias') has PT grad norm 2.1705673702854256e-08 and flax grad norm 3.484242583340347e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'k_proj', 'bias') has PT grad norm 2.0168124947872457e-08 and flax grad norm 2.791090025766607e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'k_proj', 'bias') has PT grad norm 2.427025513895842e-08 and flax grad norm 2.9163302883716824e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'k_proj', 'bias') has PT grad norm 2.230924955881619e-08 and flax grad norm 3.749363486349466e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'k_proj', 'bias') has PT grad norm 2.1171688402432665e-08 and flax grad norm 2.3053223330293804e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'k_proj', 'bias') has PT grad norm 1.8209625807230623e-08 and flax grad norm 3.436320383798375e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'k_proj', 'bias') has PT grad norm 2.9881718432989146e-08 and flax grad norm 3.781262236657312e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'k_proj', 'bias') has PT grad norm 2.35290809058597e-08 and flax grad norm 4.081080007267701e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'k_proj', 'bias') has PT grad norm 4.4978172297760466e-08 and flax grad norm 5.286420545758119e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'k_proj', 'bias') has PT grad norm 3.8322170325955085e-08 and flax grad norm 1.1843412295320377e-07.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'k_proj', 'bias') has PT grad norm 4.5850764962551693e-08 and flax grad norm 5.550143100663263e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'k_proj', 'bias') has PT grad norm 4.7812289238891026e-08 and flax grad norm 8.046924193649829e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'k_proj', 'bias') has PT grad norm 3.1326192129199626e-07 and flax grad norm 3.305707423351123e-07.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'k_proj', 'bias') has PT grad norm 4.904214634393611e-08 and flax grad norm 6.351464776344073e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '23', 'attention', 'k_proj', 'bias') has PT grad norm 4.2194997718070226e-08 and flax grad norm 8.283147678866953e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'k_proj', 'bias') has PT grad norm 6.392731677351549e-08 and flax grad norm 8.249504901414184e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'k_proj', 'bias') has PT grad norm 6.08673502711099e-08 and flax grad norm 9.036523351824144e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'k_proj', 'bias') has PT grad norm 4.105591600023217e-08 and flax grad norm 5.783982359730544e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'k_proj', 'bias') has PT grad norm 2.9147861368983286e-07 and flax grad norm 7.128102197384578e-07.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'k_proj', 'bias') has PT grad norm 2.146412114711893e-08 and flax grad norm 4.9837147741982335e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'k_proj', 'bias') has PT grad norm 1.9166110476476206e-08 and flax grad norm 2.4150972777192692e-08.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'k_proj', 'bias') has PT grad norm 2.0378276843757703e-08 and flax grad norm 2.598891057914443e-08.\n",
      "❌ Layer ('encoder', 'masked_spec_embed') has PT grad norm 0.0 and flax grad norm 0.0.\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking rel gradients match--------------------------\")\n",
    "\n",
    "if len(results_fail_rel) == 0:\n",
    "    print(\"✅ All rel grads pass\")\n",
    "else:\n",
    "    print(\"\\n\".join(results_fail_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee9b5b-a35c-44e8-9c08-7feb0e4e1775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
