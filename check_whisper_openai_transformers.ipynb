{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c1a557-c5f1-4954-a916-8c6960e63c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the 'official' whisper package made compatible with Transformers from https://github.com/patrickvonplaten/whisper\n",
    "#!pip install git+https://github.com/patrickvonplaten/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0aab94f-2425-47f8-a1b0-1b987d8088ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration, WhisperTokenizer, WhisperProcessor\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Union, List, Any\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd5c52-29a8-4f96-b697-8f07ac22f67d",
   "metadata": {},
   "source": [
    "### OpenAI Whisper Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087da296-2a27-4aab-ad74-ab16b1250675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pad_to_mel(array):\n",
    "    \"\"\"Static function which:\n",
    "        1. Pads/trims a list of audio arrays to a max length of 30s\n",
    "        2. Computes log-mel filter coefficients from padded/trimmed audio sequences\n",
    "        Inputs:\n",
    "            array: list of audio arrays\n",
    "        Returns:\n",
    "            input_ids: torch.tensor of log-mel filter bank coefficients\n",
    "    \"\"\"\n",
    "    padded_input = whisper.pad_or_trim(np.asarray(array, dtype=np.float32))\n",
    "    input_ids = whisper.log_mel_spectrogram(padded_input)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OpenAIWhisperDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the audio inputs received. An EOS token is appended to the labels sequences.\n",
    "    They are then dynamically padded to max length.\n",
    "    Args:\n",
    "        eos_token_id (`int`)\n",
    "            The end-of-sentence token for the Whisper tokenizer. Ensure to set for sequences to terminate before\n",
    "            generation max length.\n",
    "    \"\"\"\n",
    "\n",
    "    eos_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Since Whisper models don't have a HF processor defined (feature extractor + tokenizer), we'll pad by hand...\n",
    "        \"\"\"\n",
    "        # split inputs and labels since they have to be of different lengths\n",
    "        # and need different padding methods\n",
    "        input_ids = [feature[\"input_ids\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # first, pad the audio inputs to max_len\n",
    "        input_ids = torch.concat([to_pad_to_mel(input_val)[None, :] for input_val in input_ids])\n",
    "\n",
    "        # next, append the eos token to our sequence of labels\n",
    "        labels = [lab + [self.eos_token_id] for lab in labels]\n",
    "        # finally, pad the target labels to max_len\n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        max_label_len = max(label_lengths)\n",
    "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
    "\n",
    "        batch = {\"labels\": labels}\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e345a8-f692-4381-aa3d-c7f392f53f47",
   "metadata": {},
   "source": [
    "### Transformers Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de68d298-e827-4f0e-91e6-0a50526d2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`WhisperProcessor`])\n",
    "            The processor used for processing the data.\n",
    "        decoder_start_token_id (`int`)\n",
    "            The begin-of-sentence of the decoder.\n",
    "        eos_token_id (`int`)\n",
    "            The end-of-sentence of the model.\n",
    "        model_input_name (`str`)\n",
    "            Name of the pre-processed audio inputs expected by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    eos_token_id: int\n",
    "    model_input_name: str\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need different padding methods\n",
    "        # first treat the audio inputs by padding to max length\n",
    "        input_features = [{self.model_input_name: feature[self.model_input_name]} for feature in features]\n",
    "        \n",
    "        # this does nothing for Whisper models where the inputs are already padded to max length in the audio input space\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # now handle the target labels\n",
    "        for feature in features:\n",
    "            # if bos token is prepended in previous tokenization step,\n",
    "            # cut bos token here as it's prepended later anyways\n",
    "            if feature[\"labels\"][0] == self.decoder_start_token_id:\n",
    "                feature[\"labels\"] = feature[\"labels\"][1:]\n",
    "            # if eos token is not appended in previous tokenization step,\n",
    "            # append eos token here as it's not appended later\n",
    "            if feature[\"labels\"][-1] != self.eos_token_id and self.eos_token_id is not None:\n",
    "                feature[\"labels\"].append(self.eos_token_id)\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2809051-9ea2-4bab-b90e-af366966b59f",
   "metadata": {},
   "source": [
    "### Load OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2d1728-c90b-405e-8cda-578cb7d3d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_whisper = whisper.load_model(\"tiny.en\")\n",
    "\n",
    "openai_tok = whisper.tokenizer.get_tokenizer(False, task=\"transcribe\", language=\"en\")\n",
    "openai_tokenizer = openai_tok.tokenizer\n",
    "openai_tokenizer.pad_token = openai_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7b548-edd8-406f-b646-a465f1e52e96",
   "metadata": {},
   "source": [
    "### Load Transformers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051b88b1-f9b5-408d-84be-dce035354600",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_whisper = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor()\n",
    "feature_extractor.return_attention_mask = False\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "eos_token_id = transformers_whisper.config.eos_token_id\n",
    "decoder_start_token_id = transformers_whisper.config.decoder_start_token_id\n",
    "model_input_name = feature_extractor.model_input_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522a4ce-9d9b-418e-a94c-d472e39b8cf1",
   "metadata": {},
   "source": [
    "### Load Dummy Dataset and Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed458097-276e-4c2d-87fd-682ecf9a1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/Users/sanchitgandhi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "vectorized_dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6b4c3a-1d06-4d34-83dd-6c185781041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_column_name = \"audio\"\n",
    "text_column_name = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a9d25e-61d4-44fd-a1b2-936d11433398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_openai_dataset(batch):\n",
    "    # pre-process audio\n",
    "    sample = batch[audio_column_name]\n",
    "\n",
    "    # For training OpenAI Whisper we perform the audio preprocessing in the OpenAIWhisperDataCollator\n",
    "    # => we only need to supply it with the raw audio values\n",
    "    batch[\"input_ids\"] = sample[\"array\"]\n",
    "    batch[\"input_lengths\"] = len(batch[\"input_ids\"])\n",
    "\n",
    "    input_str = batch[text_column_name].lower()\n",
    "    batch[\"labels\"] = openai_tokenizer(input_str).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "def prepare_transformers_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.get(model_input_name)[0]\n",
    "    batch[\"input_length\"] = len(sample[\"array\"])\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name].lower()\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc501be1-4f14-480b-91bb-0e1d2b8b7c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e3658210f74d02836212be95b2c83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openai_dataset = vectorized_dataset.map(prepare_openai_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a65412-990b-417e-a74f-361cc31b7f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a154ad812a58469eb0b0419841d7f52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanchitgandhi/venv/lib/python3.8/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "transformers_dataset = vectorized_dataset.map(prepare_transformers_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea119b-3396-4371-987d-6d47a4a34149",
   "metadata": {},
   "source": [
    "### Check Equality of Data Collator Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e442ee-1ff5-4994-acaf-1981caa9106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_collator = OpenAIWhisperDataCollatorWithPadding(eos_token_id=openai_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46ccf5d0-870f-4d3d-9120-bccfc571a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    model_input_name=model_input_name,\n",
    "    eos_token_id=eos_token_id,\n",
    "    decoder_start_token_id=decoder_start_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f55e5f1-44a9-4834-83a8-8f88412b4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db381a6-fedb-4dc8-a93c-bf1055b6cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_inputs = openai_collator([openai_dataset[i] for i in range(batch_size)])\n",
    "transformers_inputs = transformers_collator([transformers_dataset[i] for i in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "375676b9-e1f1-4441-87f8-f9328bfea38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8253e-05)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(openai_inputs[\"input_ids\"] - transformers_inputs[\"input_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05f26e5b-c725-4f12-bd6c-b503b52742c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(openai_inputs[\"labels\"] == transformers_inputs[\"labels\"]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210f9bf-827d-4e7b-8250-1f799356ad8c",
   "metadata": {},
   "source": [
    "### Check Equality of Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6822a69-8e96-457a-b14d-9c19add940ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_outputs = openai_whisper(**openai_inputs)\n",
    "transformers_outputs = transformers_whisper(**transformers_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad51637e-f2db-4a2d-af60-696d0306a3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.3505, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3505, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_outputs.loss, transformers_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271ade5e-c48e-4afc-bb0c-bdb405e80fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.4414e-05, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.abs(openai_outputs[\"logits\"] - transformers_outputs[\"logits\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186570dc-41cc-47bd-993f-8e73e238f14d",
   "metadata": {},
   "source": [
    "### Check Equality of Generation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84b2bbc1-a4c0-4de5-b1f4-840020a237b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_pred_ids = openai_whisper.generate(openai_inputs[\"input_ids\"], max_length=40)\n",
    "transformers_pred_ids = transformers_whisper.generate(transformers_inputs[\"input_features\"], max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8571e093-d36b-4d21-bdda-79e648a9102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_pred_str = openai_tokenizer.batch_decode(openai_pred_ids, skip_special_tokens=True)\n",
    "transformers_pred_str = tokenizer.batch_decode(transformers_pred_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6abbb1c-e5cd-4f1d-8bc3-095dee5ef7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI:        Linnell's pictures are a sort of up-guards-in-item paintings, and Mason's exquisite idles are as national as a jingo poem. Mr. Birkett Foster\n",
      "Transformers:  Linnell's pictures are a sort of upguards and atom paintings, and Mason's exquisite idles are as national as a jingo poem. Mr. Birkett Foster's landscapes smile\n"
     ]
    }
   ],
   "source": [
    "for i, (openai_pred_id, transformers_pred_id) in enumerate(zip(openai_pred_ids, transformers_pred_ids)):\n",
    "    if not (openai_pred_id == transformers_pred_id).all():\n",
    "        print(f\"OpenAI:       {openai_pred_str[i]}\\nTransformers: {transformers_pred_str[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce64b5-69bf-4aa3-ace4-61d97316b040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
