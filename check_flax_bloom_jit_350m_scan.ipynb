{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03f1ec8-3ca3-4edc-9c99-c6bae9177387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAX platform to CPU for highest matmul precision\n",
    "import os\n",
    "#os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d71739-61a0-470d-9b17-2e4a60c21530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxBloomForCausalLM, BloomForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9738731-61f3-49fc-86e5-0b36ee6249a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████| 748/748 [00:00<00:00, 574kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████| 1.04G/1.04G [00:11<00:00, 97.8MB/s]\n",
      "tcmalloc: large alloc 1118437376 bytes == 0x1215de000 @  0x7f9741f04680 0x7f9741f25824 0x5f8a01 0x648cf1 0x5c4676 0x4f290e 0x64f718 0x5048b3 0x56b1da 0x56939a 0x50aaa0 0x56c28c 0x56939a 0x68d047 0x6003a4 0x5c4a40 0x56b0ae 0x5002d8 0x56cadf 0x5002d8 0x56cadf 0x5002d8 0x503fb6 0x56b1da 0x5f6836 0x56b0ae 0x5f6836 0x56b1da 0x56939a 0x5f6a13 0x50aa2c\n",
      "Some of the weights of FlaxBloomForCausalLM were initialized in float16 precision from the model checkpoint at sanchit-gandhi/bloom-350m-scan:\n",
      "[('transformer', 'h', 'FlaxBloomBlockLayers', 'input_layernorm', 'bias'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'input_layernorm', 'scale'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'mlp', 'dense_4h_to_h', 'bias'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'mlp', 'dense_4h_to_h', 'kernel'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'mlp', 'dense_h_to_4h', 'bias'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'mlp', 'dense_h_to_4h', 'kernel'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'post_attention_layernorm', 'bias'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'post_attention_layernorm', 'scale'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'self_attention', 'dense', 'bias'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'self_attention', 'dense', 'kernel'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'self_attention', 'query_key_value', 'bias'), ('transformer', 'h', 'FlaxBloomBlockLayers', 'self_attention', 'query_key_value', 'kernel'), ('transformer', 'ln_f', 'bias'), ('transformer', 'ln_f', 'scale'), ('transformer', 'word_embeddings', 'embedding'), ('transformer', 'word_embeddings_layernorm', 'bias'), ('transformer', 'word_embeddings_layernorm', 'scale')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    }
   ],
   "source": [
    "#model_id = \"bigscience/bigscience-small-testing\"\n",
    "model_id = \"bigscience/bloom-350m\"\n",
    "scan_model_id = \"sanchit-gandhi/bloom-350m-scan\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-350m\")\n",
    "\n",
    "pt_model = BloomForCausalLM.from_pretrained(model_id)\n",
    "flax_model = FlaxBloomForCausalLM.from_pretrained(scan_model_id, use_scan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f18a5b-2b5f-4487-b8c0-ae6302557d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "input_str = [10*\"hello this string is definitely longer\", \"Hey you\"]\n",
    "\n",
    "inputs_pt = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs_np = tokenizer(input_str, return_tensors=\"np\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07d646a-ae15-4057-ab23-8f94f40b0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits_pt = pt_model(**inputs_pt).logits\n",
    "    logits_pt_single = pt_model(inputs_pt.input_ids[:1]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d780c61e-696b-45e4-b47b-f85bff466710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 13:34:14.872740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched padded pt vs padded flax\n",
      "1.8187866\n",
      "batched full pt vs full flax\n",
      "4.532715\n",
      "single pt vs flax\n",
      "4.8477783\n",
      "single flax vs flax\n",
      "2.32901\n"
     ]
    }
   ],
   "source": [
    "# default matmul precision (bfloat16)\n",
    "logits_fx = flax_model(**inputs_np).logits\n",
    "logits_fx_single = flax_model(inputs_np.input_ids[:1]).logits\n",
    "\n",
    "print(\"batched padded pt vs padded flax\")\n",
    "print(np.max(np.abs(logits_pt[1, :2].numpy() - np.array(logits_fx[1, :2]))))\n",
    "\n",
    "print(\"batched full pt vs full flax\")\n",
    "print(np.max(np.abs(logits_pt[0].numpy() - np.array(logits_fx[0]))))\n",
    "\n",
    "print(\"single pt vs flax\")\n",
    "print(np.max(np.abs(logits_pt_single[0].numpy() - np.array(logits_fx_single[0]))))\n",
    "\n",
    "print(\"single flax vs flax\")\n",
    "print(np.max(np.abs(np.array(logits_fx[0]) - np.array(logits_fx_single[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b30af3-6948-453d-8810-7982c858d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched padded pt vs padded flax\n",
      "0.0031738281\n",
      "batched full pt vs full flax\n",
      "0.005065918\n",
      "single pt vs flax\n",
      "0.0045776367\n",
      "single flax vs flax\n",
      "0.0004272461\n"
     ]
    }
   ],
   "source": [
    "# highest matmul precision (float32)\n",
    "with jax.default_matmul_precision('float32'):\n",
    "    logits_fx = flax_model(**inputs_np).logits\n",
    "    logits_fx_single = flax_model(inputs_np.input_ids[:1]).logits\n",
    "    \n",
    "print(\"batched padded pt vs padded flax\")\n",
    "print(np.max(np.abs(logits_pt[1, :2].numpy() - np.array(logits_fx[1, :2]))))\n",
    "\n",
    "print(\"batched full pt vs full flax\")\n",
    "print(np.max(np.abs(logits_pt[0].numpy() - np.array(logits_fx[0]))))\n",
    "\n",
    "print(\"single pt vs flax\")\n",
    "print(np.max(np.abs(logits_pt_single[0].numpy() - np.array(logits_fx_single[0]))))\n",
    "\n",
    "print(\"single flax vs flax\")\n",
    "print(np.max(np.abs(np.array(logits_fx[0]) - np.array(logits_fx_single[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769a6da-d641-4367-9f65-2e0852c88c2b",
   "metadata": {},
   "source": [
    "## JIT the fprop and watch the magic happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80292b8d-734a-4d17-8a6c-3e3a3fd18aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def flax_model_jitted(input_ids, attention_mask=None, **kwargs):\n",
    "    return flax_model(input_ids, attention_mask=attention_mask, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cb44ed-08b1-45c0-b52a-5975465da054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2236956672 bytes == 0x2d2610000 @  0x7f9741f04680 0x7f9741f25824 0x58f8b8 0x586650 0x5869d4 0x619464 0x6195b6 0x6217b3 0x5042cb 0x56b1da 0x5f6836 0x570035 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547\n",
      "tcmalloc: large alloc 2236956672 bytes == 0x1ef80c000 @  0x7f9741f04680 0x7f9741f24ff4 0x7f95d16381de 0x7f95d163a979 0x7f95d1670533 0x7f95d164f991 0x5f3989 0x5f3e1e 0x50b183 0x56c28c 0x5f6836 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x56b0ae 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547\n",
      "tcmalloc: large alloc 2146123776 bytes == 0x4fe574000 @  0x7f9741f04680 0x7f9741f24ff4 0x7f95d54228ca 0x7f95d46f3cb7 0x7f95d46e9e17 0x7f95d46e3249 0x7f95d382a611 0x7f95d3838ad0 0x7f95d188aeaa 0x7f95d166ff56 0x7f95d1670597 0x7f95d164f991 0x5f3989 0x5f3e1e 0x50b183 0x56c28c 0x5f6836 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x56b0ae 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a\n",
      "tcmalloc: large alloc 2146164736 bytes == 0x5fb232000 @  0x7f9741f04680 0x7f9741f25824 0x7f95d035ba1a 0x7f95c948d5a7 0x7f95d542297b 0x7f95d46f3cb7 0x7f95d46e9e17 0x7f95d46e3249 0x7f95d382a611 0x7f95d3838ad0 0x7f95d188aeaa 0x7f95d166ff56 0x7f95d1670597 0x7f95d164f991 0x5f3989 0x5f3e1e 0x50b183 0x56c28c 0x5f6836 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x56b0ae 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836\n",
      "tcmalloc: large alloc 2146164736 bytes == 0x5fb232000 @  0x7f9741f04680 0x7f9741f24ff4 0x7f95d5422ed9 0x7f95d46f3d46 0x7f95d46e9e17 0x7f95d46e3249 0x7f95d382a611 0x7f95d3838ad0 0x7f95d188aeaa 0x7f95d166ff56 0x7f95d1670597 0x7f95d164f991 0x5f3989 0x5f3e1e 0x50b183 0x56c28c 0x5f6836 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x56b0ae 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 13 s, total: 2min 20s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "# microbench jit compile time for batch\n",
    "%time logits_fx = flax_model_jitted(**inputs_np).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf45ce7a-625b-47a8-acf5-a678170020eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 ms, sys: 1.02 ms, total: 4.79 ms\n",
      "Wall time: 11.4 ms\n"
     ]
    }
   ],
   "source": [
    "# microbench compiled fprop -> should be ~ms, if on the order of seconds inidicates a recompilation\n",
    "%time logits_fx = flax_model_jitted(**inputs_np).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cd67dc-868c-42f9-88b9-814e3b06cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2236956672 bytes == 0x1e380a000 @  0x7f9741f04680 0x7f9741f25824 0x58f8b8 0x586650 0x5869d4 0x619464 0x6195b6 0x6217b3 0x5042cb 0x56b1da 0x5f6836 0x570035 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 6.46 s, total: 1min 38s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "# microbench jit compile time for single input\n",
    "%time logits_fx_single = flax_model_jitted(inputs_np.input_ids[:1]).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d244938-3d66-4a5f-8bfb-e6ed6328fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.17 ms, sys: 694 µs, total: 3.87 ms\n",
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "# microbench compiled fprop for single input -> should be ~ms, if on the order of seconds inidicates a recompilation\n",
    "%time logits_fx_single = flax_model_jitted(inputs_np.input_ids[:1]).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57134e64-16c4-4539-a4cd-5b18b86a70cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched padded pt vs padded flax\n",
      "1.8187866\n",
      "batched full pt vs full flax\n",
      "4.532715\n",
      "single pt vs flax\n",
      "4.8477783\n",
      "single flax vs flax\n",
      "2.32901\n"
     ]
    }
   ],
   "source": [
    "# verify correctness of jit-compiled fprop\n",
    "print(\"batched padded pt vs padded flax\")\n",
    "print(np.max(np.abs(logits_pt[1, :2].numpy() - np.array(logits_fx[1, :2]))))\n",
    "\n",
    "print(\"batched full pt vs full flax\")\n",
    "print(np.max(np.abs(logits_pt[0].numpy() - np.array(logits_fx[0]))))\n",
    "\n",
    "print(\"single pt vs flax\")\n",
    "print(np.max(np.abs(logits_pt_single[0].numpy() - np.array(logits_fx_single[0]))))\n",
    "\n",
    "print(\"single flax vs flax\")\n",
    "print(np.max(np.abs(np.array(logits_fx[0]) - np.array(logits_fx_single[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ccb1b-4ed0-4cd9-abbb-0040c2e2cf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
