{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a372b1-cc1d-47f3-8c97-2fdaf6822679",
   "metadata": {},
   "source": [
    "### 1. Set the JAX platform (CPU/TPU) and matmul precision (if on TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3d10e4-758c-4fed-b6b5-c77224909e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "#os.environ[\"JAX_DEFAULT_MATMUL_PRECISION\"]=\"float32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646118e-3707-4388-92ba-e62a8acd4180",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad6f9f2-47f9-45e3-8ef7-80b3eba34304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2Model, FlaxWav2Vec2Model, SpeechEncoderDecoderModel, FlaxSpeechEncoderDecoderModel\n",
    "from flax.traverse_util import flatten_dict\n",
    "import random\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500ae2c-676d-40a6-8f66-ca0abbce345b",
   "metadata": {},
   "source": [
    "### 3. Load pretrained 'tiny random' models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbf7505-4e19-4f75-850f-df2e35e21435",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_id = \"hf-internal-testing/tiny-random-wav2vec2\"\n",
    "decoder_id = \"hf-internal-testing/tiny-random-bart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e395750-6b9a-4e5e-9df4-5d8bb90fc48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing FlaxWav2Vec2Model: {('project_hid', 'bias'), ('project_q', 'kernel'), ('quantizer', 'codevectors'), ('project_hid', 'kernel'), ('lm_head', 'kernel'), ('quantizer', 'weight_proj', 'kernel'), ('project_q', 'bias'), ('lm_head', 'bias'), ('quantizer', 'weight_proj', 'bias')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxWav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: {('feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing FlaxWav2Vec2Model: {('project_hid', 'bias'), ('project_q', 'kernel'), ('quantizer', 'codevectors'), ('project_hid', 'kernel'), ('lm_head', 'kernel'), ('quantizer', 'weight_proj', 'kernel'), ('project_q', 'bias'), ('lm_head', 'bias'), ('quantizer', 'weight_proj', 'bias')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxWav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: {('feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-bart were not used when initializing FlaxBartForCausalLM: {('decoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'scale'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('classification_head', 'dense', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '1', 'fc1', 'bias'), ('decoder', 'layers', '1', 'fc2', 'bias'), ('model', 'shared', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('shared', 'kernel'), ('classification_head', 'dense', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('decoder', 'embed_positions', 'embedding'), ('decoder', 'layers', '0', 'fc2', 'kernel'), ('classification_head', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('decoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'embed_tokens', 'embedding'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layernorm_embedding', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'fc1', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layernorm_embedding', 'bias'), ('encoder', 'embed_tokens', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('qa_outputs', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layernorm_embedding', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc1', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'embed_positions', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('final_logits_bias',), ('model', 'encoder', 'layernorm_embedding', 'bias'), ('decoder', 'layernorm_embedding', 'scale'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'scale'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'fc1', 'bias'), ('model', 'encoder', 'layers', '1', 'fc2', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'scale'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('qa_outputs', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'scale'), ('lm_head', 'kernel'), ('decoder', 'layers', '1', 'final_layer_norm', 'scale'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('model', 'encoder', 'embed_positions', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel'), ('classification_head', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'embed_tokens', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc1', 'bias'), ('model', 'encoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'scale'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-04 11:00:19.051221: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "fx_enc_model = FlaxWav2Vec2Model.from_pretrained(encoder_id, from_pt=True)\n",
    "fx_enc_dec_model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_from_pt=True, decoder_from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51878304-e1d0-4a4d-9a33-194c499a7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing Wav2Vec2Model: ['project_hid.weight', 'project_q.bias', 'project_q.weight', 'project_hid.bias', 'lm_head.bias', 'quantizer.weight_proj.bias', 'lm_head.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing Wav2Vec2Model: ['project_hid.weight', 'project_q.bias', 'project_q.weight', 'project_hid.bias', 'lm_head.bias', 'quantizer.weight_proj.bias', 'lm_head.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-bart were not used when initializing BartForCausalLM: ['qa_outputs.weight', 'decoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.1.fc1.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.fc2.weight', 'qa_outputs.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'decoder.layernorm_embedding.bias', 'decoder.layers.0.fc2.weight', 'model.encoder.layers.0.fc1.bias', 'decoder.layers.1.fc1.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.0.fc1.weight', 'decoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc1.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.0.fc1.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.bias', 'classification_head.dense.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'encoder.layers.1.fc1.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.fc2.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'classification_head.out_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.bias', 'final_logits_bias', 'encoder.layers.1.fc2.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'encoder.layers.0.fc2.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.embed_tokens.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'decoder.layernorm_embedding.weight', 'encoder.layernorm_embedding.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layernorm_embedding.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'classification_head.out_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.fc1.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'classification_head.dense.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.embed_tokens.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layernorm_embedding.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'encoder.layers.1.fc1.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'shared.weight', 'encoder.embed_positions.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.embed_tokens.weight', 'model.shared.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.fc2.bias', 'model.encoder.layers.1.fc2.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pt_enc_model = Wav2Vec2Model.from_pretrained(encoder_id)\n",
    "pt_enc_dec_model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3887a-e43e-4a80-be5f-a98f9a820ba6",
   "metadata": {},
   "source": [
    "### 4. Check that the weights of the FlaxWav2Vec2 model match those of the FlaxSpeechEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2120c142-244a-4800-8e82-d91a922f5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's easier to work with flattened dictionaries of parameters\n",
    "fx_enc_params = flatten_dict(fx_enc_model.params)\n",
    "fx_enc_dec_params = flatten_dict(fx_enc_dec_model.params['encoder'])  # require just the encoder parameters\n",
    "\n",
    "# Check that all keys match\n",
    "assert fx_enc_params.keys() == fx_enc_dec_params.keys()\n",
    "\n",
    "# Check that all the weights are **precisely** equal - verifies that the encoder module is loaded correctly into the SpeechEncoderDecoder framework\n",
    "for param in fx_enc_params:\n",
    "    assert (fx_enc_params[param] == fx_enc_dec_params[param]).all(), param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb8d23-ffb8-4757-9b40-2eb27f74d0d3",
   "metadata": {},
   "source": [
    "### 5. Check that the weights of the SpeechEncoderDecoderModel match those of the FlaxSpeechEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40542c7e-471e-4695-ac22-80e3f0e0bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /tmp/tmpegm4xyev were not used when initializing FlaxSpeechEncoderDecoderModel: {('decoder', 'lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Convert the PT model to FX \n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_enc_dec_model.save_pretrained(tmpdirname)\n",
    "    pt_enc_dec_model_to_fx = FlaxSpeechEncoderDecoderModel.from_pretrained(tmpdirname, from_pt=True)\n",
    "    \n",
    "pt_params_to_fx = flatten_dict(pt_enc_dec_model_to_fx.params)\n",
    "fx_enc_dec_params = flatten_dict(fx_enc_dec_model.params)  # need all the parameters this time, not just encoder\n",
    "\n",
    "# Check that all keys match\n",
    "assert fx_enc_dec_params.keys() == pt_params_to_fx.keys()\n",
    "\n",
    "# Check that all the weights are **precisely** equal\n",
    "for param in pt_params_to_fx:\n",
    "    assert (fx_enc_dec_params[param] == pt_params_to_fx[param]).all(), param\n",
    "    \n",
    "# Free CPU memory \n",
    "del fx_enc_params, fx_enc_dec_params, pt_enc_dec_model_to_fx, pt_params_to_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a9ef-ba83-4772-af85-41e1b8def77d",
   "metadata": {},
   "source": [
    "### 6. Create synthetic input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39476ffb-a424-4f89-8eea-31d559e153b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_tensor(shape, vocab_size, rng=None):\n",
    "    \"\"\"Creates a random int32 tensor of the shape within the vocab size.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    output = np.array(values).reshape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def random_attention_mask(shape, rng=None):\n",
    "    attn_mask = ids_tensor(shape, vocab_size=2, rng=rng)\n",
    "    # make sure that at least one token is attended to for each batch\n",
    "    attn_mask[:, -1] = 1\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "def floats_tensor(shape, scale=1.0, rng=None):\n",
    "    \"\"\"Creates a random float32 tensor\"\"\"\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.random() * scale)\n",
    "\n",
    "    return np.array(values, dtype=np.float32).reshape(shape)\n",
    "\n",
    "\n",
    "def shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = np.zeros_like(input_ids)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    shifted_input_ids = np.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f955c7e5-0127-427f-a5c4-0bc34d5cf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "encoder_input_length = 5000\n",
    "decoder_input_length = 16\n",
    "\n",
    "input_ids = floats_tensor([batch_size, encoder_input_length])\n",
    "attention_mask = random_attention_mask([batch_size, encoder_input_length])\n",
    "label_ids = ids_tensor([batch_size, decoder_input_length], fx_enc_dec_model.config.decoder.vocab_size)\n",
    "decoder_input_ids = shift_tokens_right(input_ids=label_ids, pad_token_id=fx_enc_dec_model.config.decoder.pad_token_id,\n",
    "                                       decoder_start_token_id=fx_enc_dec_model.config.decoder.decoder_start_token_id or fx_enc_dec_model.config.decoder.bos_token_id)\n",
    "\n",
    "fx_inputs = {\n",
    "    \"inputs\": input_ids,\n",
    "    \"decoder_input_ids\": decoder_input_ids\n",
    "}\n",
    "\n",
    "pt_inputs = {k: torch.tensor(v.tolist()) for k, v in fx_inputs.items()}\n",
    "pt_inputs[\"labels\"] = torch.tensor(label_ids.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc62ea-6096-4501-9902-c97fb24bb6b2",
   "metadata": {},
   "source": [
    "### 7. FlaxWav2Vec2Model vs FlaxSpeechEncoderDecoderModel's encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f112375-8a84-4b25-b7df-add62d2b7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the FlaxWav2Vec2Model outputs to those of the FlaxSpeechEncoderDecoderModel's encoder - they should be equal!\n",
    "fx_enc_outputs = fx_enc_model(fx_inputs[\"inputs\"], output_hidden_states=True)\n",
    "fx_enc_dec_outputs = fx_enc_dec_model(**fx_inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e58903-75c8-4ec0-ad5d-bdddab8b5cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['last_hidden_state', 'extract_features', 'hidden_states']),\n",
       " odict_keys(['logits', 'decoder_hidden_states', 'encoder_last_hidden_state', 'encoder_hidden_states']))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_enc_outputs.keys(), fx_enc_dec_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28826810-ec97-4a8c-8fa5-67b7263729d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function for our analysis\n",
    "def assert_almost_equals(a: np.ndarray, b: np.ndarray, tol: float = 1e-2):\n",
    "    diff = np.abs((a - b)).max()\n",
    "    if diff < tol:\n",
    "        print(f\"✅ Difference between Flax and PyTorch is {diff} (< {tol})\")\n",
    "    else:\n",
    "        print(f\"❌ Difference between Flax and PyTorch is {diff} (>= {tol})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d7220e-4aaf-46c8-9e7f-e6cf84f29d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder hidden states match--------------------------\n",
      "❌ Difference between Flax and PyTorch is 0.408400297164917 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 0.41299885511398315 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 0.4098670184612274 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 0.41270017623901367 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n",
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "Encoder-decoder output shape: (2, 76, 16), encoder-only output shape: (2, 76, 16)\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder hidden states match--------------------------\")\n",
    "for fx_enc_dec_state, fx_enc_state in zip(fx_enc_dec_outputs.encoder_hidden_states, fx_enc_outputs.hidden_states):\n",
    "    assert fx_enc_dec_state.shape == fx_enc_state.shape\n",
    "    assert_almost_equals(fx_enc_dec_state, fx_enc_state)\n",
    "\n",
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "print(f\"Encoder-decoder output shape: {fx_enc_dec_outputs.encoder_last_hidden_state.shape}, encoder-only output shape: {fx_enc_outputs.last_hidden_state.shape}\")\n",
    "assert_almost_equals(fx_enc_dec_outputs.encoder_last_hidden_state, fx_enc_outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae984d39-391c-4f46-b6b5-111d292a454f",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "The above results clearly demonstrate that there is something wrong with how the encoder module is called in the FlaxSpeechEncoderDecoderModel. Let's probe a little deeper by comparing:\n",
    "\n",
    "8. **PyTorch Encoder vs PyTorch Encoder-Decoder Outputs** - assert that the encoder is loaded correctly in PyTorch and highlight the results we expect to see in Flax ✅\n",
    "9. **Flax Encoder vs PyTorch Encoder Outputs** - assert that the Wav2Vec2 Model is correctly implemented in Flax ✅\n",
    "10. **Flax Encoder-Decoder vs PyTorch Encoder-Decoder Outputs** - sanity check to make sure the same results hold when we compare the same hidden-state values across PyTorch and Flax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfcb010-e0fd-45a9-aff0-e4838ce1dece",
   "metadata": {},
   "source": [
    "### 8. Wav2Vec2Model vs SpeechEncoderDecoderModel's encoder module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95dc5b02-ff67-4bb3-b1ae-4692def7fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the outputs of the PyTorch Wav2Vec2 encoder only model with the output of the PyTorch SpeechEncoderDecoder's encoder module - they should be equal!\n",
    "pt_enc_outputs = pt_enc_model(pt_inputs[\"inputs\"], output_hidden_states=True)\n",
    "pt_enc_dec_outputs = pt_enc_dec_model(**pt_inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f218fd-f3a5-4177-92e9-36806d106f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder hidden states match--------------------------\n",
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01)\n",
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "Encoder-decoder output shape: torch.Size([2, 76, 16]), encoder-only output shape: torch.Size([2, 76, 16])\n",
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder hidden states match--------------------------\")\n",
    "for pt_enc_dec_state, pt_enc_state in zip(pt_enc_dec_outputs.encoder_hidden_states, pt_enc_outputs.hidden_states):\n",
    "    assert pt_enc_dec_state.shape == pt_enc_state.shape\n",
    "    assert_almost_equals(pt_enc_dec_state.detach().numpy(), pt_enc_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "print(f\"Encoder-decoder output shape: {pt_enc_dec_outputs.encoder_last_hidden_state.shape}, encoder-only output shape: {pt_enc_outputs.last_hidden_state.shape}\")\n",
    "assert_almost_equals(pt_enc_dec_outputs.encoder_last_hidden_state.detach().numpy(), pt_enc_outputs.last_hidden_state.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f74ff3-8509-4b5c-a3b8-7f420f6ab891",
   "metadata": {},
   "source": [
    "### 9. FlaxWav2Vec2Model vs Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce75c5ea-fdf4-4aee-975d-7da238ea6b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking extract features match--------------------------\n",
      "✅ Difference between Flax and PyTorch is 4.0531158447265625e-06 (< 0.01)\n",
      "--------------------------Checking encoder hidden states match--------------------------\n",
      "✅ Difference between Flax and PyTorch is 3.501772880554199e-07 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 3.501772880554199e-07 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 3.5390257835388184e-07 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 3.5390257835388184e-07 (< 0.01)\n",
      "✅ Difference between Flax and PyTorch is 3.7550926208496094e-06 (< 0.01)\n",
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "Encoder-decoder output shape: (2, 76, 16), encoder-only output shape: torch.Size([2, 76, 16])\n",
      "✅ Difference between Flax and PyTorch is 3.7550926208496094e-06 (< 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Compare the outputs of the FlaxWav2Vec2Model with the PyTorch Wav2Vec2Model \n",
    "\n",
    "print(\"--------------------------Checking extract features match--------------------------\")\n",
    "assert_almost_equals(fx_enc_outputs.extract_features, pt_enc_outputs.extract_features.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking encoder hidden states match--------------------------\")\n",
    "for fx_enc_state, pt_enc_state in zip(fx_enc_outputs.hidden_states, pt_enc_outputs.hidden_states):\n",
    "    assert fx_enc_state.shape == pt_enc_state.shape\n",
    "    assert_almost_equals(fx_enc_state, pt_enc_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "print(f\"Encoder-decoder output shape: {fx_enc_outputs.last_hidden_state.shape}, encoder-only output shape: {pt_enc_outputs.last_hidden_state.shape}\")\n",
    "assert_almost_equals(fx_enc_outputs.last_hidden_state, pt_enc_outputs.last_hidden_state.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203aac1-f6d0-4e8b-a4f3-3140824d4754",
   "metadata": {},
   "source": [
    "### 10. FlaxSpeechEncoderDecoderModel's encoder module vs SpeechEncoderDecoderModel's encoder module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22e50f0-e455-4ffd-8680-b5971b6fa362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder hidden states match--------------------------\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n",
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "Encoder-decoder output shape: (2, 76, 16), encoder-only output shape: torch.Size([2, 76, 16])\n",
      "❌ Difference between Flax and PyTorch is 2.5930871963500977 (>= 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Compare the outputs of the FlaxSpeechEncoderDecoderModel's encoder outputs to that of the PyTorch SpeechEncoderDecoderModel's encoder\n",
    "\n",
    "print(\"--------------------------Checking encoder hidden states match--------------------------\")\n",
    "for fx_enc_dec_state, pt_enc_state in zip(fx_enc_dec_outputs.encoder_hidden_states, pt_enc_dec_outputs.encoder_hidden_states):\n",
    "    assert fx_enc_dec_state.shape == pt_enc_dec_state.shape\n",
    "    assert_almost_equals(fx_enc_dec_state, pt_enc_dec_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "print(f\"Encoder-decoder output shape: {fx_enc_dec_outputs.encoder_last_hidden_state.shape}, encoder-only output shape: {pt_enc_dec_outputs.encoder_last_hidden_state.shape}\")\n",
    "assert_almost_equals(fx_enc_dec_outputs.encoder_last_hidden_state, pt_enc_dec_outputs.encoder_last_hidden_state.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e1a8d-2b30-4431-b7a2-da54085781bd",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490d592-c8ee-42ad-8d21-34dfbc63129a",
   "metadata": {},
   "source": [
    "1. The Flax encoder module weights are loaded correctly into the speech encoder-decoder model framework.\n",
    "2. The Flax speech encoder-decoder model weights are identical to those of their PyTorch counterpart.\n",
    "3. The Flax encoder model outputs do not match those of the encoder-decoder model's encoder outputs for randomly distributed inputs - there is something wrong with the way the encoder module is implemented in the Flax speech encoder-decoder model framework!\n",
    "4. The PyTorch encoder model outputs match those of the PyTorch encoder-decoder model's encoder outputs - the encoder module is correctly implemented in the PyTorch speech encoder-decoder model framework.\n",
    "5. The Flax encoder model outputs match those of the PyTorch encoder model outputs - the Wav2Vec2 model is correctly implemented in Flax.\n",
    "6. The Flax encoder-decoder outputs do not match those of the PyTorch encoder-decoder outputs - further evidence there is something wrong with the Flax encoder-decoder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3533a-1a1b-4126-8ee4-19d0dc1ee968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
