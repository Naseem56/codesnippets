{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4084e2c-1c5d-4cbc-95c6-bcd63e5c3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio, load_dataset\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65d646-3693-4b34-9308-c98f8ae41f13",
   "metadata": {},
   "source": [
    "## Single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014e25b0-6cab-462b-9875-f4967314aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"validation\", streaming=True)\n",
    "dataset = dataset.take(32)\n",
    "\n",
    "whisper_asr = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-tiny.en\", device=0\n",
    ")\n",
    "\n",
    "whisper_asr.model.config.suppress_tokens.remove(6)\n",
    "whisper_asr.model.config.suppress_tokens.remove(12)\n",
    "\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f80caf-aac0-4bf6-99f6-7dc15599d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cast = dataset.cast_column(\"audio\", Audio(16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbe08c5-d337-44f1-95be-33e7b4c9e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: get the column names for the datasets\n",
    "def get_text(sample):\n",
    "    if \"text\" in sample:\n",
    "        return sample[\"text\"]\n",
    "    elif \"sentence\" in sample:\n",
    "        return sample[\"sentence\"]\n",
    "    elif \"normalized_text\" in sample:\n",
    "        return sample[\"normalized_text\"]\n",
    "    elif \"transcript\" in sample:\n",
    "        return sample[\"transcript\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Sample: {sample.keys()} has no transcript.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e880a279-0d4a-45bb-83f5-5cb34d646306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(batch):\n",
    "    batch[\"norm_text\"] = whisper_asr.tokenizer._normalize(get_text(batch))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7661fb53-7e43-4bdb-8bc1-4a4c932914ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_norm = dataset_cast.map(normalise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a91d880-6f6a-41a3-b8b5-f3ea7080520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_target_text_in_range(ref):\n",
    "    if ref.strip() == \"ignore time segment in scoring\":\n",
    "        return False\n",
    "    else:\n",
    "        return ref.strip() != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d74d3b5-652b-4a72-9c16-8423f085a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filter = dataset_norm.filter(is_target_text_in_range, input_columns=[\"norm_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7a7506-82ca-43c9-8a6d-551189d59013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(my_dataset):\n",
    "    for i, sample in enumerate(my_dataset):\n",
    "        yield sample[\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6326dfe-c16b-45c2-9d97-74f7bd1ae554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 309 ms, total: 16.4 s\n",
      "Wall time: 3.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for out in whisper_asr(data(dataset_filter), batch_size=8):\n",
    "    predictions.append(whisper_asr.tokenizer._normalize((out[\"text\"])))\n",
    "    \n",
    "dataset_text = dataset_filter.remove_columns(\"audio\")\n",
    "\n",
    "for i, sample in enumerate(dataset_text):\n",
    "    references.append(sample[\"norm_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "620f9534-2400-4acb-82ef-9a83ddd07976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9458850056369785"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * wer_metric.compute(references=references, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7ae55-b157-49ee-a722-09e8eafae239",
   "metadata": {},
   "source": [
    "## Multi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "975cacff-2fd7-47a1-97c4-6a8fc8a40abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_clean = load_dataset(\"librispeech_asr\", \"all\", split=\"test.clean\", streaming=True)\n",
    "librispeech_other = load_dataset(\"librispeech_asr\", \"all\", split=\"test.other\", streaming=True)\n",
    "\n",
    "common_voice = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", revision=\"streaming\", split=\"test\", streaming=True, use_auth_token=True)\n",
    "\n",
    "voxpopuli = load_dataset(\"facebook/voxpopuli\", \"en\", split=\"test\", streaming=True)\n",
    "\n",
    "tedlium = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"test\", streaming=True)\n",
    "\n",
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs\", split=\"test\", streaming=True, use_auth_token=True)\n",
    "\n",
    "spgispeech = load_dataset(\"kensho/spgispeech\", \"S\", split=\"test\", streaming=True, use_auth_token=True)\n",
    "\n",
    "#earnings22 = load_dataset(\"anton-l/earnings22_baseline_5_gram\", split=\"test\", streaming=True)\n",
    "\n",
    "ami = load_dataset(\"edinburghcstr/ami\", \"ihm\", split=\"test\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2158f25f-b349-4af9-9ca5-a2aa3cb2f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "esb_datasets = {\"LibriSpeech Clean\": librispeech_clean,\n",
    "                \"LibriSpeech Other\": librispeech_other,\n",
    "                \"Common Voice\": common_voice,\n",
    "                \"VoxPopuli\": voxpopuli,\n",
    "                \"TEDLIUM\": tedlium,\n",
    "                \"GigaSpeech\": gigaspeech,\n",
    "                \"SPGISpeech\": spgispeech,\n",
    "                #\"Earnings-22\": earnings22,\n",
    "                \"AMI\": ami}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daec5ed0-285f-469b-8ab5-23e899a5e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 16354it [00:00, 51617.76it/s]\n",
      "Reading metadata...: 16354it [00:00, 74394.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 s, sys: 1.16 s, total: 48.8 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# batch size for extracting references and predictions\n",
    "batch_size = 4\n",
    "\n",
    "wer_results = []\n",
    "\n",
    "# loop over all the datasets in the ESB benchmark\n",
    "for dataset_name, dataset in esb_datasets.items():    \n",
    "    # first 32 samples\n",
    "    dataset = dataset.take(8)\n",
    "\n",
    "    # resample to 16kHz\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "    # normalise references\n",
    "    dataset = dataset.map(normalise)\n",
    "\n",
    "    # remove any empty references\n",
    "    dataset = dataset.filter(is_target_text_in_range, input_columns=[\"norm_text\"])\n",
    "\n",
    "    # run streamed inference\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for out in whisper_asr(data(dataset), batch_size=batch_size):\n",
    "        predictions.append(whisper_asr.tokenizer._normalize((out[\"text\"])))\n",
    "\n",
    "    dataset = dataset.remove_columns(\"audio\")\n",
    "\n",
    "    for i, sample in enumerate(dataset):\n",
    "        references.append(sample[\"norm_text\"])\n",
    "\n",
    "    # compute the WER\n",
    "    wer = wer_metric.compute(references=references, predictions=predictions)\n",
    "    wer = round(100 * wer, 2)\n",
    "\n",
    "    wer_results.append(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f00a9-89ea-40a7-8367-da62def1c070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
