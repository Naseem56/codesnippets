{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio git+https://github.com/huggingface/transformers torch soundfile librosa pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWzz1XzFNQxi",
        "outputId": "91a85847-43bc-46c6-a48f-8808f83ecfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-dkrrhu0g\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-dkrrhu0g\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.8/dist-packages (3.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.26.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.26.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.8/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (from gradio) (0.88.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from gradio) (3.16.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.8/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: paramiko in /usr/local/lib/python3.8/dist-packages (from gradio) (2.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.8/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.8/dist-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: markdown-it-py[linkify,plugins] in /usr/local/lib/python3.8/dist-packages (from gradio) (2.1.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: starlette==0.22.0 in /usr/local/lib/python3.8/dist-packages (from fastapi->gradio) (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (3.6.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi->gradio) (1.3.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (0.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.8/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.6)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.8/dist-packages (from paramiko->gradio) (38.0.4)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from paramiko->gradio) (1.5.0)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.8/dist-packages (from paramiko->gradio) (4.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Installing collected packages: pytube\n",
            "Successfully installed pytube-12.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Processor + Batch Decode\n",
        "\n",
        "Cuts the audio off after 30 seconds"
      ],
      "metadata": {
        "id": "wcWmafvJ373t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytube as pt\n",
        "import torch\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "yt = pt.YouTube(\"https://www.youtube.com/watch?v=dd1kN_myNDs\")\n",
        "stream = yt.streams.filter(only_audio=True)[0]\n",
        "stream.download(filename=\"audio.mp3\")\n",
        "\n",
        "MODEL_NAME = \"openai/whisper-small.en\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def load_and_fix_data(input_file):  \n",
        "    speech, sample_rate = librosa.load(input_file)\n",
        "    if len(speech.shape) > 1: \n",
        "        speech = speech[:,0] + speech[:,1]\n",
        "    if sample_rate !=16000:\n",
        "        speech = librosa.resample(speech, sample_rate,16000)\n",
        "    speech = librosa.to_mono(speech)\n",
        "    return speech\n",
        "\n",
        "\n",
        "def transcribe(Microphone, File_Upload=None):\n",
        "    warn_output = \"\"\n",
        "    if (Microphone is not None) and (File_Upload is not None):\n",
        "        warn_output = \"WARNING: You've uploaded an audio file and used the microphone. \" \\\n",
        "                      \"The recorded file from the microphone will be used and the uploaded audio will be discarded.\\n\"\n",
        "        file = Microphone\n",
        "\n",
        "    elif (Microphone is None) and (File_Upload is None):\n",
        "        return \"ERROR: You have to either use the microphone or upload an audio file\"\n",
        "\n",
        "    elif Microphone is not None:\n",
        "        file = Microphone\n",
        "    else:\n",
        "        file = File_Upload\n",
        "    \n",
        "    speech_data = load_and_fix_data(file)\n",
        "\n",
        "    inputs = processor(speech_data, return_tensors=\"pt\", sampling_rate=16_000).input_features.to(device)\n",
        "#    forced_decoder_ids = processor.get_decoder_prompt_ids(language=lang, task=\"transcribe\")\n",
        "\n",
        "    predicted_ids = model.generate(inputs, max_length=480)\n",
        "    text = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)[0]    \n",
        "\n",
        "    return warn_output + text\n",
        "\n",
        "transcribe(\"audio.mp3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Bpq7ikxT1llK",
        "outputId": "32ed6f37-be44-4af1-da89-35582078a7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dear fellow scholars this is 2 minute papers with karoly zsolnai feher there are many ai techniques that are able to look at a still image and identify objects textures human poses and object parts in them really well however in the age of the internet we have videos everywhere so an important question would be how we could do the same for these animations'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Pipeline\n",
        "\n",
        "Chunks quite alright!"
      ],
      "metadata": {
        "id": "MqRr0TlP6bOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytube as pt\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "yt = pt.YouTube(\"https://www.youtube.com/watch?v=dd1kN_myNDs\")\n",
        "stream = yt.streams.filter(only_audio=True)[0]\n",
        "stream.download(filename=\"audio.mp3\")\n",
        "\n",
        "MODEL_NAME = \"openai/whisper-small.en\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = pipeline(task=\"automatic-speech-recognition\", model=MODEL_NAME, chunk_length_s=30)\n",
        "\n",
        "def load_and_fix_data(input_file):  \n",
        "    speech, sample_rate = librosa.load(input_file)\n",
        "    if len(speech.shape) > 1: \n",
        "        speech = speech[:,0] + speech[:,1]\n",
        "    if sample_rate !=16000:\n",
        "        speech = librosa.resample(speech, sample_rate,16000)\n",
        "    speech = librosa.to_mono(speech)\n",
        "    return speech\n",
        "\n",
        "\n",
        "def transcribe(Microphone, File_Upload=None):\n",
        "    warn_output = \"\"\n",
        "    if (Microphone is not None) and (File_Upload is not None):\n",
        "        warn_output = \"WARNING: You've uploaded an audio file and used the microphone. \" \\\n",
        "                      \"The recorded file from the microphone will be used and the uploaded audio will be discarded.\\n\"\n",
        "        file = Microphone\n",
        "\n",
        "    elif (Microphone is None) and (File_Upload is None):\n",
        "        return \"ERROR: You have to either use the microphone or upload an audio file\"\n",
        "\n",
        "    elif Microphone is not None:\n",
        "        file = Microphone\n",
        "    else:\n",
        "        file = File_Upload\n",
        "    \n",
        "    speech_data = load_and_fix_data(file)\n",
        "    text = pipe(speech_data) \n",
        "\n",
        "    return warn_output + text[\"text\"]\n",
        "\n",
        "transcribe(\"audio.mp3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "FDaNu3ZT1vif",
        "outputId": "8bfe2418-4c62-4061-a10b-dad515ad3de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True)\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are many AI techniques that are able to look at a still image and identify objects, textures, human poses, and object parts in them really well. However, in the age of the Internet, we have videos everywhere, so an important question would be how we could do the same for these animations. One of the key ideas in this paper is that the frames of these videos are not completely independent, and they share a lot of information, so after we make our initial predictions on what is where exactly, these predictions from the previous frame can almost always be reused with a little modification. Not only that, but here you can see with these results that it can also deal with momentary occlusions and is ready to track objects that rotate or time. A key part of this method is that one, it looks back and forth in these videos to update these labels, and second, it learns in a self-supervised manner, which means that all it is given is just a little more than data, and was never given a nice dataset with explicit labels of these regions and objects parts that it could learn from. You can see in this comparison table that this is not the only method that works for videos, the paper contains ample comparisons against other methods, and comes out ahead of all other unsupervised methods, and on this task, it can even get quite close to supervised methods. The supervised methods are the ones that have access to these cushy, labeled datasets, and therefore should come out way ahead. But they don't, which sounds like witchcraft, considering that this technique is learning on its own. However, all this greatness comes with limitations. One of the bigger ones is that even though it does extremely considering that this technique is learning on its own. and for your generous support, and I'll see you next time!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "2i2iG_BUNCx9",
        "outputId": "c9304595-6a4e-4e26-9b04-415faa7b6c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gradio/inputs.py:319: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  \"Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n",
            "/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.7/dist-packages/gradio/interface.py:332: UserWarning: Currently, only the 'default' theme is supported.\n",
            "  warnings.warn(\"Currently, only the 'default' theme is supported.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://aab2b77e7afe7f5d.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aab2b77e7afe7f5d.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "MODEL_NAME = \"openai/whisper-small\"\n",
        "lang = \"ja\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "def load_and_fix_data(input_file):  \n",
        "    speech, sample_rate = librosa.load(input_file)\n",
        "    if len(speech.shape) > 1: \n",
        "        speech = speech[:,0] + speech[:,1]\n",
        "    if sample_rate !=16000:\n",
        "        speech = librosa.resample(speech, sample_rate,16000)\n",
        "    speech = librosa.to_mono(speech)\n",
        "    return speech\n",
        "\n",
        "\n",
        "def transcribe(Microphone, File_Upload):\n",
        "    warn_output = \"\"\n",
        "    if (Microphone is not None) and (File_Upload is not None):\n",
        "        warn_output = \"WARNING: You've uploaded an audio file and used the microphone. \" \\\n",
        "                      \"The recorded file from the microphone will be used and the uploaded audio will be discarded.\\n\"\n",
        "        file = Microphone\n",
        "\n",
        "    elif (Microphone is None) and (File_Upload is None):\n",
        "        return \"ERROR: You have to either use the microphone or upload an audio file\"\n",
        "\n",
        "    elif Microphone is not None:\n",
        "        file = Microphone\n",
        "    else:\n",
        "        file = File_Upload\n",
        "    \n",
        "    speech_data = load_and_fix_data(file)\n",
        "\n",
        "    inputs = processor(speech_data, return_tensors=\"pt\", sampling_rate=16_000).input_features.to(device)\n",
        "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=lang, task=\"transcribe\")\n",
        "\n",
        "    predicted_ids = model.generate(inputs, max_length=480_000, forced_decoder_ids=forced_decoder_ids)\n",
        "    text = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)[0]    \n",
        "\n",
        "    return warn_output + text\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type='filepath', optional=True),\n",
        "        gr.inputs.Audio(source=\"upload\", type='filepath', optional=True),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    layout=\"horizontal\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"[WFTE] Whisper model showcase\",\n",
        "    description=\"Demo for showcasing fine-tuned OpenAI Whisper models from WFTE.\",\n",
        "    allow_flagging='never',\n",
        ")\n",
        "\n",
        "iface.launch(enable_queue=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "MODEL_NAME = \"openai/whisper-small\"\n",
        "lang = \"ja\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def load_and_fix_data(input_file):  \n",
        "    speech, sample_rate = librosa.load(input_file)\n",
        "    if len(speech.shape) > 1: \n",
        "        speech = speech[:,0] + speech[:,1]\n",
        "    if sample_rate !=16000:\n",
        "        speech = librosa.resample(speech, sample_rate,16000)\n",
        "    speech = librosa.to_mono(speech)\n",
        "    return speech\n",
        "\n",
        "\n",
        "def transcribe(Microphone, File_Upload=None):\n",
        "    warn_output = \"\"\n",
        "    if (Microphone is not None) and (File_Upload is not None):\n",
        "        warn_output = \"WARNING: You've uploaded an audio file and used the microphone. \" \\\n",
        "                      \"The recorded file from the microphone will be used and the uploaded audio will be discarded.\\n\"\n",
        "        file = Microphone\n",
        "\n",
        "    elif (Microphone is None) and (File_Upload is None):\n",
        "        return \"ERROR: You have to either use the microphone or upload an audio file\"\n",
        "\n",
        "    elif Microphone is not None:\n",
        "        file = Microphone\n",
        "    else:\n",
        "        file = File_Upload\n",
        "    \n",
        "    speech_data = load_and_fix_data(file)\n",
        "\n",
        "    inputs = processor(speech_data, return_tensors=\"pt\", sampling_rate=16000).input_features.to(device)\n",
        "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=lang, task=\"transcribe\")\n",
        "\n",
        "    predicted_ids = model.generate(inputs, max_length=480_000, forced_decoder_ids=forced_decoder_ids)\n",
        "    text = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)[0]    \n",
        "\n",
        "    return warn_output + text"
      ],
      "metadata": {
        "id": "P7DbmPclNE-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcribe(\"result.flac\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "PQyIszJIP45z",
        "outputId": "969c737a-d83a-41c3-ce85-3659f54ff5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'キムラさんに電話をかしてもらいました'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}