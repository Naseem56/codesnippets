{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176191db-e8d1-4289-81a2-13e0fbe5b4e1",
   "metadata": {},
   "source": [
    "# Freezing a Subset of Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad6f9f2-47f9-45e3-8ef7-80b3eba34304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxSpeechEncoderDecoderModel\n",
    "import random\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import traverse_util\n",
    "from flax.training import train_state\n",
    "from flax.core import freeze, frozen_dict\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from flax.training.common_utils import onehot\n",
    "from typing import Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500ae2c-676d-40a6-8f66-ca0abbce345b",
   "metadata": {},
   "source": [
    "## 1. Load a pre-trained 'tiny' speech encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbf7505-4e19-4f75-850f-df2e35e21435",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_id = \"hf-internal-testing/tiny-random-wav2vec2\"\n",
    "decoder_id = \"hf-internal-testing/tiny-random-bart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e395750-6b9a-4e5e-9df4-5d8bb90fc48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 were not used when initializing FlaxWav2Vec2Model: {('quantizer', 'weight_proj', 'bias'), ('lm_head', 'kernel'), ('project_hid', 'kernel'), ('project_q', 'kernel'), ('lm_head', 'bias'), ('project_hid', 'bias'), ('quantizer', 'weight_proj', 'kernel'), ('project_q', 'bias'), ('quantizer', 'codevectors')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxWav2Vec2Model were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: {('feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias'), ('feature_extractor', 'conv_layers', '2', 'layer_norm', 'scale'), ('feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at hf-internal-testing/tiny-random-bart were not used when initializing FlaxBartForCausalLM: {('model', 'encoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'final_layer_norm', 'scale'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('model', 'encoder', 'layers', '1', 'fc1', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('classification_head', 'dense', 'bias'), ('final_logits_bias',), ('decoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('decoder', 'layernorm_embedding', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel'), ('classification_head', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'encoder', 'layernorm_embedding', 'bias'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'embed_positions', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias'), ('model', 'encoder', 'embed_tokens', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'fc2', 'kernel'), ('decoder', 'embed_tokens', 'embedding'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'scale'), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('model', 'encoder', 'layers', '1', 'fc2', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'encoder_attn_layer_norm', 'scale'), ('decoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('qa_outputs', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('model', 'shared', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'fc1', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'scale'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel'), ('encoder', 'layernorm_embedding', 'kernel'), ('model', 'encoder', 'embed_positions', 'kernel'), ('model', 'encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'embed_positions', 'embedding'), ('model', 'encoder', 'layernorm_embedding', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '0', 'fc2', 'bias'), ('encoder', 'embed_tokens', 'kernel'), ('qa_outputs', 'kernel'), ('decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('classification_head', 'dense', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('shared', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'final_layer_norm', 'scale'), ('model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'scale'), ('classification_head', 'out_proj', 'kernel'), ('lm_head', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias'), ('decoder', 'layernorm_embedding', 'scale'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('model', 'encoder', 'layers', '0', 'fc2', 'bias'), ('model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias'), ('encoder', 'layernorm_embedding', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('model', 'encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'fc2', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('model', 'encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-11 09:15:35.328693: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_from_pt=True, decoder_from_pt=True)\n",
    "params = freeze(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f73917-b0f5-4f7d-8a7b-2652fbb96621",
   "metadata": {},
   "source": [
    "## 2. Utility functions for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d273f4b-d45f-474f-afc6-ee93e63b8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(d, depth=0, print_value=False):\n",
    "    for k in d.keys():\n",
    "        if isinstance(d[k], FrozenDict):\n",
    "            print('  ' * depth, k)\n",
    "            print_tree(d[k], depth + 1, print_value)\n",
    "        else:\n",
    "            if print_value:\n",
    "                print('  ' * depth, k, d[k])\n",
    "            else:\n",
    "                print('  ' * depth, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50c1783-fdc5-4ede-b568-3cb925a11883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_params(lhs, rhs, depth=0):\n",
    "    for k in lhs.keys():\n",
    "        if isinstance(lhs[k], FrozenDict):\n",
    "            print('  ' * depth, k)\n",
    "            compare_params(lhs[k], rhs[k], depth + 1)\n",
    "        else:\n",
    "            print('  ' * depth, k, jnp.mean(jnp.abs(lhs[k] - rhs[k])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb39e9-989f-4fed-9ede-0846f1a5906f",
   "metadata": {},
   "source": [
    "## 3. View the parameter tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afc1ef9-8a95-470f-8b82-ea0f9b24048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " decoder\n",
      "   model\n",
      "     decoder\n",
      "       embed_tokens\n",
      "         embedding\n",
      "       embed_positions\n",
      "         embedding\n",
      "       layers\n",
      "         0\n",
      "           self_attn\n",
      "             k_proj\n",
      "               kernel\n",
      "               bias\n",
      "             v_proj\n",
      "               kernel\n",
      "               bias\n",
      "             q_proj\n",
      "               kernel\n",
      "               bias\n",
      "             out_proj\n",
      "               kernel\n",
      "               bias\n",
      "           self_attn_layer_norm\n",
      "             scale\n",
      "             bias\n",
      "           encoder_attn\n",
      "             k_proj\n",
      "               kernel\n",
      "               bias\n",
      "             v_proj\n",
      "               kernel\n",
      "               bias\n",
      "             q_proj\n",
      "               kernel\n",
      "               bias\n",
      "             out_proj\n",
      "               kernel\n",
      "               bias\n",
      "           encoder_attn_layer_norm\n",
      "             scale\n",
      "             bias\n",
      "           fc1\n",
      "             kernel\n",
      "             bias\n",
      "           fc2\n",
      "             kernel\n",
      "             bias\n",
      "           final_layer_norm\n",
      "             scale\n",
      "             bias\n",
      "         1\n",
      "           self_attn\n",
      "             k_proj\n",
      "               kernel\n",
      "               bias\n",
      "             v_proj\n",
      "               kernel\n",
      "               bias\n",
      "             q_proj\n",
      "               kernel\n",
      "               bias\n",
      "             out_proj\n",
      "               kernel\n",
      "               bias\n",
      "           self_attn_layer_norm\n",
      "             scale\n",
      "             bias\n",
      "           encoder_attn\n",
      "             k_proj\n",
      "               kernel\n",
      "               bias\n",
      "             v_proj\n",
      "               kernel\n",
      "               bias\n",
      "             q_proj\n",
      "               kernel\n",
      "               bias\n",
      "             out_proj\n",
      "               kernel\n",
      "               bias\n",
      "           encoder_attn_layer_norm\n",
      "             scale\n",
      "             bias\n",
      "           fc1\n",
      "             kernel\n",
      "             bias\n",
      "           fc2\n",
      "             kernel\n",
      "             bias\n",
      "           final_layer_norm\n",
      "             scale\n",
      "             bias\n",
      "       layernorm_embedding\n",
      "         scale\n",
      "         bias\n",
      " encoder\n",
      "   masked_spec_embed\n",
      "   feature_extractor\n",
      "     conv_layers\n",
      "       0\n",
      "         conv\n",
      "           kernel\n",
      "         layer_norm\n",
      "           scale\n",
      "           bias\n",
      "       1\n",
      "         conv\n",
      "           kernel\n",
      "         layer_norm\n",
      "           bias\n",
      "           scale\n",
      "       2\n",
      "         conv\n",
      "           kernel\n",
      "         layer_norm\n",
      "           bias\n",
      "           scale\n",
      "   feature_projection\n",
      "     layer_norm\n",
      "       scale\n",
      "       bias\n",
      "     projection\n",
      "       kernel\n",
      "       bias\n",
      "   encoder\n",
      "     pos_conv_embed\n",
      "       conv\n",
      "         bias\n",
      "         weight_g\n",
      "         weight_v\n",
      "     layer_norm\n",
      "       scale\n",
      "       bias\n",
      "     layers\n",
      "       0\n",
      "         attention\n",
      "           k_proj\n",
      "             kernel\n",
      "             bias\n",
      "           v_proj\n",
      "             kernel\n",
      "             bias\n",
      "           q_proj\n",
      "             kernel\n",
      "             bias\n",
      "           out_proj\n",
      "             kernel\n",
      "             bias\n",
      "         layer_norm\n",
      "           scale\n",
      "           bias\n",
      "         feed_forward\n",
      "           intermediate_dense\n",
      "             kernel\n",
      "             bias\n",
      "           output_dense\n",
      "             kernel\n",
      "             bias\n",
      "         final_layer_norm\n",
      "           scale\n",
      "           bias\n",
      "       1\n",
      "         attention\n",
      "           k_proj\n",
      "             kernel\n",
      "             bias\n",
      "           v_proj\n",
      "             kernel\n",
      "             bias\n",
      "           q_proj\n",
      "             kernel\n",
      "             bias\n",
      "           out_proj\n",
      "             kernel\n",
      "             bias\n",
      "         layer_norm\n",
      "           scale\n",
      "           bias\n",
      "         feed_forward\n",
      "           intermediate_dense\n",
      "             kernel\n",
      "             bias\n",
      "           output_dense\n",
      "             kernel\n",
      "             bias\n",
      "         final_layer_norm\n",
      "           scale\n",
      "           bias\n",
      "       2\n",
      "         attention\n",
      "           k_proj\n",
      "             kernel\n",
      "             bias\n",
      "           v_proj\n",
      "             kernel\n",
      "             bias\n",
      "           q_proj\n",
      "             kernel\n",
      "             bias\n",
      "           out_proj\n",
      "             kernel\n",
      "             bias\n",
      "         layer_norm\n",
      "           scale\n",
      "           bias\n",
      "         feed_forward\n",
      "           intermediate_dense\n",
      "             kernel\n",
      "             bias\n",
      "           output_dense\n",
      "             kernel\n",
      "             bias\n",
      "         final_layer_norm\n",
      "           scale\n",
      "           bias\n",
      "       3\n",
      "         attention\n",
      "           k_proj\n",
      "             kernel\n",
      "             bias\n",
      "           v_proj\n",
      "             kernel\n",
      "             bias\n",
      "           q_proj\n",
      "             kernel\n",
      "             bias\n",
      "           out_proj\n",
      "             kernel\n",
      "             bias\n",
      "         layer_norm\n",
      "           scale\n",
      "           bias\n",
      "         feed_forward\n",
      "           intermediate_dense\n",
      "             kernel\n",
      "             bias\n",
      "           output_dense\n",
      "             kernel\n",
      "             bias\n",
      "         final_layer_norm\n",
      "           scale\n",
      "           bias\n"
     ]
    }
   ],
   "source": [
    "# view the param tree for the full model (note: long output!)\n",
    "print_tree(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd8b7d-80d6-4a0b-b256-bf74ca840c91",
   "metadata": {},
   "source": [
    "As we can see, the param tree is somewhat large! And this is just for a 'tiny' seq2seq model... To make our lives easier, we'll focus on two specific modules from the encoder: the `feature_extractor` and the `feature_projection`. For the former, we have added a `jax.lax.stop_gradient` operator within the [modelling code](https://github.com/huggingface/transformers/blob/d57da992371c1c8258dc683275b4711dee949d20/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L409-L410), which is triggered if the argument `freeze_feature_encoder` is set to `True`. We can use this argument to freeze the feature extractor layers during training - by action of the `jax.lax.stop_gradient` operator, the gradients of these layers are clamped to precisely zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db45b081-8133-461e-ab4e-48446155c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extractor\n",
      "   conv_layers\n",
      "     0\n",
      "       conv\n",
      "         kernel\n",
      "       layer_norm\n",
      "         scale\n",
      "         bias\n",
      "     1\n",
      "       conv\n",
      "         kernel\n",
      "       layer_norm\n",
      "         bias\n",
      "         scale\n",
      "     2\n",
      "       conv\n",
      "         kernel\n",
      "       layer_norm\n",
      "         bias\n",
      "         scale\n"
     ]
    }
   ],
   "source": [
    "print(\"feature_extractor\")\n",
    "print_tree(params['encoder']['feature_extractor'], depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd9c6d3-1488-49c4-aa02-c97979881384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_projection\n",
      "   layer_norm\n",
      "     scale\n",
      "     bias\n",
      "   projection\n",
      "     kernel\n",
      "     bias\n"
     ]
    }
   ],
   "source": [
    "print(\"feature_projection\")\n",
    "print_tree(params['encoder']['feature_projection'], depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9ca10-2883-411d-89d5-13bf3b2b1440",
   "metadata": {},
   "source": [
    "Lets get an idea of how many of the model parameters we are freezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b941a20-8d10-48eb-aa82-2841cf7a1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 49368, num frozen params: 16832, proportion of model params frozen: 34.1%\n"
     ]
    }
   ],
   "source": [
    "# get num params\n",
    "num_params = sum(jax.tree_leaves(jax.tree_map(lambda x: jnp.size(x), params)))\n",
    "# get num frozen params\n",
    "num_frozen_params = sum(jax.tree_leaves(jax.tree_map(lambda x: jnp.size(x), params['encoder']['feature_extractor'])))\n",
    "# accumulate statistics\n",
    "prop_frozen_params = num_frozen_params / num_params * 100\n",
    "print(f\"Num params: {num_params}, num frozen params: {num_frozen_params}, proportion of model params frozen: {prop_frozen_params:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2a9ef-ba83-4772-af85-41e1b8def77d",
   "metadata": {},
   "source": [
    "## 4. Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39476ffb-a424-4f89-8eea-31d559e153b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_tensor(shape, vocab_size, key):\n",
    "    \"\"\"Creates a random int32 tensor of the shape within the vocab size.\"\"\"\n",
    "    return jax.random.randint(key, shape, 0, vocab_size - 1)\n",
    "\n",
    "\n",
    "def random_attention_mask(shape, key):\n",
    "    \"\"\"Creates a random binary int32 tensor.\"\"\"\n",
    "    attn_mask = ids_tensor(shape, vocab_size=2, key=key)\n",
    "    # make sure that at least one token is attended to for each batch\n",
    "    attn_mask = attn_mask.at[:, -1].set(1)\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "def floats_tensor(shape, key, scale=1.0):\n",
    "    \"\"\"Creates a random float32 tensor.\"\"\"\n",
    "    return jax.random.normal(key, shape=shape) * scale\n",
    "\n",
    "\n",
    "def shift_tokens_right(input_ids: jnp.array, pad_token_id: int, decoder_start_token_id: int) -> jnp.ndarray:\n",
    "    \"\"\"Shifts input ids one token to the right.\"\"\"\n",
    "    shifted_input_ids = jnp.zeros_like(input_ids)\n",
    "    shifted_input_ids = shifted_input_ids.at[:, 1:].set(input_ids[:, :-1])\n",
    "    shifted_input_ids = shifted_input_ids.at[:, 0].set(decoder_start_token_id)\n",
    "\n",
    "    shifted_input_ids = jnp.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f955c7e5-0127-427f-a5c4-0bc34d5cf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(key, batch_size=2, encoder_input_length=96000, decoder_input_length=31):\n",
    "    \"\"\"Function for creating a dummy batch of data of specified dimensions.\"\"\"\n",
    "    input_ids = floats_tensor([batch_size, encoder_input_length], key)\n",
    "    attention_mask = random_attention_mask([batch_size, encoder_input_length], key)\n",
    "    label_ids = ids_tensor([batch_size, decoder_input_length], model.config.decoder.vocab_size, key)\n",
    "    decoder_input_ids = shift_tokens_right(input_ids=label_ids, pad_token_id=model.config.decoder.pad_token_id,\n",
    "                                           decoder_start_token_id=model.config.decoder.decoder_start_token_id or model.config.decoder.bos_token_id)\n",
    "\n",
    "    batch_inputs = {\n",
    "        \"inputs\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"decoder_input_ids\": decoder_input_ids,\n",
    "        \"labels\": label_ids,\n",
    "    }\n",
    "    return batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c10a2f-06ea-44c2-b20f-29a6e93f5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# create batches of synthetic input data\n",
    "inputs = []\n",
    "for i in range(1000):\n",
    "    key, rng = jax.random.split(rng, num=2)\n",
    "    inputs.append(create_batch(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fab8e-aa35-40fb-9f6d-c3d5b68df9a4",
   "metadata": {},
   "source": [
    "## 5. Define a train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b96e12e-2a7b-4602-887d-ea5f49e90ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss function\n",
    "def loss_fn(logits, labels):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = optax.softmax_cross_entropy(logits, onehot(labels, vocab_size))\n",
    "    # ignore padded tokens from loss, i.e. where labels are not set to -100\n",
    "    padding = labels >= 0\n",
    "    loss = loss * padding\n",
    "    loss = loss.sum() / padding.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e59643-b91b-4dcd-9848-9bf37cff3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train step\n",
    "def train_step(state, batch, rng, freeze_feature_encoder=False):\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(rng)\n",
    "    \n",
    "    def compute_loss(params):\n",
    "        labels = batch.pop('labels')\n",
    "        outputs = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, freeze_feature_encoder=freeze_feature_encoder, train=True)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        return loss, outputs\n",
    "\n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, outputs), grads = grad_fn(state.params)\n",
    "    \n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    return new_state, new_dropout_rng\n",
    "\n",
    "# jit training step\n",
    "j_train_step = jax.jit(train_step, static_argnums=(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6198e-d519-41a8-9f7d-3e15e4e3ac6b",
   "metadata": {},
   "source": [
    "## 6. Optimizer without freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e65238-7657-42c1-8e1e-90cf927d62a7",
   "metadata": {},
   "source": [
    "### Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be14576-440d-4714-ac94-1568ef2d13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.adam(0.1)\n",
    "\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__,\n",
    "                                      params=params,\n",
    "                                      tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221152f-82c2-4daa-b58b-edd935584720",
   "metadata": {},
   "source": [
    "### Run one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4384b84-617c-4311-83ca-36dec550627a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state, rng = j_train_step(state, inputs[0], rng, freeze_feature_encoder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13984d94-655e-4fda-aedf-2a653826887f",
   "metadata": {},
   "source": [
    "### Compare parameters after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a915b285-1bd9-4066-bcaa-28cfaaa84bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_layers\n",
      "   0\n",
      "     conv\n",
      "       kernel 0.09710271\n",
      "     layer_norm\n",
      "       scale 0.09954791\n",
      "       bias 0.099674284\n",
      "   1\n",
      "     conv\n",
      "       kernel 0.09776986\n",
      "     layer_norm\n",
      "       bias 0.09932138\n",
      "       scale 0.099454924\n",
      "   2\n",
      "     conv\n",
      "       kernel 0.09607415\n",
      "     layer_norm\n",
      "       bias 0.099158935\n",
      "       scale 0.09879182\n"
     ]
    }
   ],
   "source": [
    "# the change for the (non-frozen) feature extractor parameters should be greater than zero\n",
    "compare_params(params['encoder']['feature_extractor'], state.params['encoder']['feature_extractor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da20d6ef-ba3b-4a76-8bf9-a286163c0bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_norm\n",
      "   scale 0.09655121\n",
      "   bias 0.09949996\n",
      " projection\n",
      "   kernel 0.099958576\n",
      "   bias 0.10006119\n"
     ]
    }
   ],
   "source": [
    "# the change for the (non-frozen) feature projection parameters should be greater than zero\n",
    "compare_params(params['encoder']['feature_projection'], state.params['encoder']['feature_projection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a37871f-a676-4107-98b7-130ef308aaea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Benchmark an non-frozen train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e17653-b709-4589-9302-111ef12e6a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 s ± 269 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for batch in inputs:\n",
    "    new_state, new_rng = j_train_step(state, batch, rng, freeze_feature_encoder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c6f78-673a-4d2c-b6a4-53b704c88ab1",
   "metadata": {},
   "source": [
    "## 7. Optimizer with `set_to_zero` and parameter mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f2db7-b05e-47f7-b698-0ce33c15eeee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a mask for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac6b9f1-2405-44ef-b867-b75430e061a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_mask_fn(params):\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    flat_mask = {path: 'zero' if path[1] == 'feature_extractor' else 'adam' for path in flat_params}\n",
    "    mask = traverse_util.unflatten_dict(flat_mask)\n",
    "    return freeze(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ea8aca1-79f8-4507-befa-405be297b9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extractor\n",
      "   conv_layers\n",
      "     0\n",
      "       conv\n",
      "         kernel zero\n",
      "       layer_norm\n",
      "         scale zero\n",
      "         bias zero\n",
      "     1\n",
      "       conv\n",
      "         kernel zero\n",
      "       layer_norm\n",
      "         bias zero\n",
      "         scale zero\n",
      "     2\n",
      "       conv\n",
      "         kernel zero\n",
      "       layer_norm\n",
      "         bias zero\n",
      "         scale zero\n",
      "feature_projection\n",
      "   layer_norm\n",
      "     scale adam\n",
      "     bias adam\n",
      "   projection\n",
      "     kernel adam\n",
      "     bias adam\n"
     ]
    }
   ],
   "source": [
    "mask = grad_mask_fn(params)\n",
    "\n",
    "# check that we are masking the right params (feature_extractor and not others)\n",
    "print(\"feature_extractor\")\n",
    "print_tree(mask['encoder']['feature_extractor'], depth=1, print_value=True)\n",
    "print(\"feature_projection\")\n",
    "print_tree(mask['encoder']['feature_projection'], depth=1, print_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04075365-abde-4712-80ab-10231743b584",
   "metadata": {},
   "source": [
    "### Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f36aa06-2cc3-4b37-a840-b57000d6da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.multi_transform({'adam': optax.adam(0.1), 'zero': optax.set_to_zero()},\n",
    "                           mask)\n",
    "\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__,\n",
    "                                      params=params,\n",
    "                                      tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144470d8-8482-47d7-9316-ae8425f8ac50",
   "metadata": {},
   "source": [
    "### Run one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b052da-a272-4f82-957f-75fe3e899840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we are freezing the layers with the grad mask and **not** with the `freeze_feature_encoder` argument\n",
    "state, rng = j_train_step(state, inputs[0], rng, freeze_feature_encoder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18657-71c0-4b6b-ad04-fd15ca1585e9",
   "metadata": {},
   "source": [
    "### Compare parameters after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d2fad96-1ba1-4165-8898-48c49a52f88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_layers\n",
      "   0\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       scale 0.0\n",
      "       bias 0.0\n",
      "   1\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       bias 0.0\n",
      "       scale 0.0\n",
      "   2\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       bias 0.0\n",
      "       scale 0.0\n"
     ]
    }
   ],
   "source": [
    "# the change for the frozen feature extractor parameters should be precisely zero\n",
    "compare_params(params['encoder']['feature_extractor'], state.params['encoder']['feature_extractor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9877b6a-3c54-48e8-be46-b54b9ba74978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_norm\n",
      "   scale 0.095603384\n",
      "   bias 0.09918707\n",
      " projection\n",
      "   kernel 0.099999696\n",
      "   bias 0.10006401\n"
     ]
    }
   ],
   "source": [
    "# the change for the (non-frozen) feature projection parameters should be greater than zero\n",
    "compare_params(params['encoder']['feature_projection'], state.params['encoder']['feature_projection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8317b3c-75c9-49b6-ade3-c3123a193c60",
   "metadata": {},
   "source": [
    "### Benchmark a `set_to_zero` train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03ab4026-982d-4c0c-b322-5c7eec5432a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5 s ± 442 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for batch in inputs:\n",
    "    new_state, new_rng = j_train_step(state, batch, rng, freeze_feature_encoder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb7ddd-a1e4-4648-b522-c3216ed95bfb",
   "metadata": {},
   "source": [
    "## 8. Optimizer with `jax.lax.stop_gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2f10e-b14d-465f-a018-c5246ada0eca",
   "metadata": {},
   "source": [
    "### Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f08845b-044a-4da7-9b8b-eab5a98547f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.adam(0.1)\n",
    "\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__,\n",
    "                                      params=params,\n",
    "                                      tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441b6bb-a917-45a4-8a17-601f2a479067",
   "metadata": {},
   "source": [
    "### Run one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4615b8d-3466-42f9-b044-e621f9df9ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state, rng = j_train_step(state, inputs[0], rng, freeze_feature_encoder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4ad0d-736d-4288-b493-96d7faa0bb75",
   "metadata": {},
   "source": [
    "### Compare parameters after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02cf6593-0c50-478b-8061-f69f374baa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_layers\n",
      "   0\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       scale 0.0\n",
      "       bias 0.0\n",
      "   1\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       bias 0.0\n",
      "       scale 0.0\n",
      "   2\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       bias 0.0\n",
      "       scale 0.0\n"
     ]
    }
   ],
   "source": [
    "# the change for the frozen feature extractor parameters should be precisely zero\n",
    "compare_params(params['encoder']['feature_extractor'], state.params['encoder']['feature_extractor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f3b4cc2-e7b2-4053-8aec-48d887e72c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_norm\n",
      "   scale 0.09889982\n",
      "   bias 0.09973503\n",
      " projection\n",
      "   kernel 0.09995212\n",
      "   bias 0.100052685\n"
     ]
    }
   ],
   "source": [
    "# the change for the (non-frozen) feature projection parameters should be greater than zero\n",
    "compare_params(params['encoder']['feature_projection'], state.params['encoder']['feature_projection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1e17e-40eb-4e5b-803d-bac612426c03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Benchmark a `jax.lax.stop_gradient` train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df96d195-10f4-4d6b-bdb1-c5b95d53495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5 s ± 243 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for batch in inputs:\n",
    "    new_state, new_rng = j_train_step(state, batch, rng, freeze_feature_encoder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1521b-c384-4ff8-8ea2-68c0c91e211a",
   "metadata": {},
   "source": [
    "## 9. Differentiate non-frozen params only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85171f8-9e77-4421-b71f-ee7db28a9ed8",
   "metadata": {},
   "source": [
    "### Define filter and merge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1855d41e-4009-47f5-b97b-d811357b0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_grad_mask_fn(params):\n",
    "    \"\"\"Create a boolean mask over parameters for filtering.\n",
    "    Returns:\n",
    "        mask (frozen_dict): `True` for non-frozen parameters, `False` for frozen parameters (i.e. the feature encoder).\n",
    "    \"\"\"\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    flat_mask = {path: path[1] != 'feature_extractor' for path in flat_params}\n",
    "    mask = traverse_util.unflatten_dict(flat_mask)\n",
    "    return freeze(mask)\n",
    "\n",
    "def filter_params(params, mask):\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    flat_mask = traverse_util.flatten_dict(mask)\n",
    "    filtered_params = {}\n",
    "    for name, value in flat_params.items():\n",
    "        if flat_mask[name]:\n",
    "            filtered_params[name] = flat_params[name]\n",
    "    return freeze(traverse_util.unflatten_dict(filtered_params))\n",
    "\n",
    "def merge_params(params: Mapping, updates: Mapping) -> dict:\n",
    "    output = params.unfreeze()\n",
    "\n",
    "    for name, update_value in updates.items():\n",
    "        current_value = params.get(name, None)\n",
    "        if isinstance(current_value, Mapping) and isinstance(update_value, Mapping):\n",
    "            output[name] = merge_params(current_value, update_value)\n",
    "        else:\n",
    "            output[name] = update_value\n",
    "\n",
    "    return freeze(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12447810-679f-4ed4-947d-f3efc60a56cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extractor\n",
      "   conv_layers\n",
      "     0\n",
      "       conv\n",
      "         kernel False\n",
      "       layer_norm\n",
      "         scale False\n",
      "         bias False\n",
      "     1\n",
      "       conv\n",
      "         kernel False\n",
      "       layer_norm\n",
      "         bias False\n",
      "         scale False\n",
      "     2\n",
      "       conv\n",
      "         kernel False\n",
      "       layer_norm\n",
      "         bias False\n",
      "         scale False\n",
      "feature_projection\n",
      "   layer_norm\n",
      "     scale True\n",
      "     bias True\n",
      "   projection\n",
      "     kernel True\n",
      "     bias True\n"
     ]
    }
   ],
   "source": [
    "# check that the masking function works as intended\n",
    "bool_mask = bool_grad_mask_fn(params)\n",
    "\n",
    "# check that we are masking the right params (feature_extractor and not others)\n",
    "print(\"feature_extractor\")\n",
    "print_tree(bool_mask['encoder']['feature_extractor'], depth=1, print_value=True)\n",
    "print(\"feature_projection\")\n",
    "print_tree(bool_mask['encoder']['feature_projection'], depth=1, print_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2539a9d5-0029-4ca6-8331-1fc4b83ca127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules for filtered encoder: frozen_dict_keys(['masked_spec_embed', 'feature_projection', 'encoder'])\n"
     ]
    }
   ],
   "source": [
    "# check that the filter function works as intended\n",
    "filtered_params = filter_params(params, bool_mask)\n",
    "\n",
    "assert 'feature_extractor' not in filtered_params['encoder'], \"Feature extractor params not filtered by filter function\"\n",
    "print(f\"Modules for filtered encoder: {filtered_params['encoder'].keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068db57-3b19-4197-bf0e-92ab6826f994",
   "metadata": {},
   "source": [
    "### Define a modified train step - only take grads of differentiable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc4fba27-a60a-4f8c-ade3-614878bf46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch, rng):\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(rng)\n",
    "    \n",
    "    def compute_loss(differentiable_params, params):\n",
    "        params = merge_params(params, differentiable_params)\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        return loss, outputs\n",
    "\n",
    "    differentiable_params = filter_params(state.params, bool_grad_mask_fn(state.params))\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, outputs), grads = grad_fn(differentiable_params, state.params)\n",
    "\n",
    "    updates, opt_state = state.tx.update(grads, state.opt_state)\n",
    "    differentiable_params = optax.apply_updates(differentiable_params, updates)\n",
    "    params = merge_params(state.params, differentiable_params)\n",
    "    \n",
    "    new_state = state.replace(\n",
    "        params=params,\n",
    "        opt_state=opt_state,\n",
    "    )\n",
    "    \n",
    "    return new_state, new_dropout_rng\n",
    "\n",
    "# jit training step\n",
    "j_train_step = jax.jit(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfc448-559d-4f4d-98d8-0391018589dd",
   "metadata": {},
   "source": [
    "### Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ef2f2bb-3994-4fea-b2eb-145a4925b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.adam(0.1)\n",
    "\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__,\n",
    "                                      params=params,\n",
    "                                      tx=tx)\n",
    "\n",
    "state = state.replace(\n",
    "    opt_state=state.tx.init(filter_params(state.params, bool_grad_mask_fn(state.params)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e9895-69df-4f01-a9cb-86e6af7742be",
   "metadata": {},
   "source": [
    "### Run one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3d151d3-6699-4ed2-af3c-710f58cc868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, rng = j_train_step(state, inputs[0], rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4c34c-52c3-4afc-adb2-664aacdc2fc2",
   "metadata": {},
   "source": [
    "### Compare parameters after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84850f37-db6e-4d81-a193-6d1080f27da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_layers\n",
      "   0\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       scale 0.0\n",
      "       bias 0.0\n",
      "   1\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       bias 0.0\n",
      "       scale 0.0\n",
      "   2\n",
      "     conv\n",
      "       kernel 0.0\n",
      "     layer_norm\n",
      "       bias 0.0\n",
      "       scale 0.0\n"
     ]
    }
   ],
   "source": [
    "# the change for the frozen feature extractor parameters should be precisely zero\n",
    "compare_params(params['encoder']['feature_extractor'], state.params['encoder']['feature_extractor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8650929d-2ffd-40cb-94aa-5847a8103c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_norm\n",
      "   scale 0.09865255\n",
      "   bias 0.09918152\n",
      " projection\n",
      "   kernel 0.09994524\n",
      "   bias 0.10005053\n"
     ]
    }
   ],
   "source": [
    "# the change for the (non-frozen) feature projection parameters should be greater than zero\n",
    "compare_params(params['encoder']['feature_projection'], state.params['encoder']['feature_projection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fda9a-c6da-4d59-9cf8-b03cba146130",
   "metadata": {},
   "source": [
    "### Benchmark a filtered train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6202c0d-36f2-4781-8014-7a3cbc4e83d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5 s ± 278 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for batch in inputs:\n",
    "    new_state, new_rng = j_train_step(state, batch, rng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
