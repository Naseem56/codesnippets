{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb02de-fcbf-41c8-9f8c-dac3f882baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit_huggingface_co/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import datasets\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a99f9d-972a-47c2-ab4f-721722cd0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset common_voice_9_0 (/home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d)\n",
      "Reusing dataset xtreme_s (/home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2)\n",
      "Reusing dataset tedlium (/home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d)\n",
      "Reusing dataset gigaspeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c)\n",
      "Using custom data configuration sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0\n",
      "Reusing dataset parquet (/home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Reusing dataset spgispeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0)\n",
      "Reusing dataset switchboard (/home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc)\n"
     ]
    }
   ],
   "source": [
    "librispeech = load_dataset(\"librispeech_asr\", \"all\", split=\"train.clean.100[:10]+train.clean.360[:10]+train.other.500[:10]\", use_auth_token=True)\n",
    "\n",
    "common_voice_9_0 = load_dataset(\"mozilla-foundation/common_voice_9_0\", \"en\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "voxpopuli = load_dataset(\"google/xtreme_s\", \"voxpopuli.en\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "tedlium = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"l\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "earnings22 = load_dataset(\"sanchit-gandhi/earnings22_robust_split\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "spgispeech = load_dataset(\"kensho/spgispeech\", \"L\", split=\"train\", use_auth_token=True, revision=\"f4d7d3b3f9b66414a09532ec937e285197afeaf6\")\n",
    "\n",
    "switchboard = load_dataset(\"ldc/switchboard\", \"switchboard\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "train_datasets = [librispeech, common_voice_9_0, voxpopuli, tedlium, gigaspeech, earnings22, spgispeech, switchboard]\n",
    "ds_name = [\"librispeech\", \"common_voice_9_0\", \"voxpopuli\", \"tedlium\", \"gigaspeech\", \"earnings22\", \"spgispeech\", \"switchboard\"]\n",
    "\n",
    "transcript_column_names = ['text', 'sentence', 'transcription', 'text', 'text', 'sentence', 'transcript', 'text']\n",
    "id_column_names = ['id', 'client_id', 'id', 'id', 'segment_id', 'source_id', 'wav_filename', 'id']\n",
    "do_lower_cases = [True, False, True, True, True, False, False, True]\n",
    "\n",
    "\n",
    "tedlium_contractions = [\" 's\", \" 't\", \" 're\", \" 've\", \" 'm\", \" 'll\", \" 'd\", \" 'clock\", \" 'all\"]\n",
    "gigaspeech_punctuation = {\" <comma>\": \",\", \" <period>\": \".\", \" <questionmark>\": \"?\", \" <exclamationpoint>\": \"!\"}\n",
    "gigaspeech_disfluencies = [\"<other>\", \"<sil>\"]\n",
    "swb_disfluencies = [\"[noise]\", \"[laughter]\", \"[silence]\", \"<a_aside>\", \"<b_aside>\", \"<e_aside>\", \"[laughter-\",\n",
    "                    \"[vocalized-noise]\", \"_1\"]\n",
    "swb_punctuations = [\"{\", \"}\", \"[\", \"]-\", \"]\"]\n",
    "earnings_disfluencies = [\"<crosstalk>\", \"<affirmative>\", \"<inaudible>\", \"inaudible\", \"<laugh>\"]\n",
    "ignore_segments = [\"ignore_time_segment_in_scoring\", \"<noise>\", \"<music>\", \"[noise]\", \"[laughter]\", \"[silence]\",\n",
    "                   \"[vocalized-noise]\", \"<crosstalk>\", \"<affirmative>\", \"<inaudible>\", \"<laugh>\", \"<other>\",\n",
    "                   \"<sil>\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd74ae-9466-4c59-843d-6212ab924d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_correction(datasets, ds_name, transcript_column_names, id_column_names, do_lower_cases):\n",
    "    for i, ds in enumerate(datasets):\n",
    "        dataset_name = ds_name[i]\n",
    "        text_column_name = transcript_column_names[i]\n",
    "        id_column_name = id_column_names[i]\n",
    "        do_lower_case = do_lower_cases[i]\n",
    "\n",
    "        if text_column_name != \"text\":\n",
    "            ds = ds.rename_column(text_column_name, \"text\")\n",
    "        if id_column_name != \"id\":\n",
    "            ds = ds.rename_column(id_column_name, \"id\")\n",
    "\n",
    "        def is_target_labels(input_str):\n",
    "            return input_str.lower() not in ignore_segments\n",
    "\n",
    "        ds = ds.filter(is_target_labels, input_columns=[\"text\"], desc=\"filtering text...\")\n",
    "\n",
    "        def prepare_dataset(batch):\n",
    "            # Pre-process audio\n",
    "            try:\n",
    "                sample = batch[\"audio\"]\n",
    "            except ValueError:\n",
    "                # E22: some samples are empty (no audio). Reading the empty audio array will trigger\n",
    "                # a soundfile ValueError. For now, we'll manually set these arrays to a zero array.\n",
    "                # They will be filtered in the subsequent filtering stage and so are\n",
    "                # explicitly ignored during training.\n",
    "                sample = {\"array\": np.array([0.]), \"sampling_rate\": 16000}\n",
    "\n",
    "            # time in s\n",
    "            batch[\"input_length\"] = len(sample[\"array\"]) / sample[\"sampling_rate\"]\n",
    "\n",
    "            # 'Error correction' of targets\n",
    "            input_str = batch[\"text\"].lower() if do_lower_case else batch[\"text\"]\n",
    "            # LibriSpeech ASR\n",
    "            if \"librispeech\" in dataset_name:\n",
    "                pass  # no error correction necessary\n",
    "            # VoxPopuli\n",
    "            if \"voxpopuli\" in dataset_name:\n",
    "                pass  # no error correction necessary\n",
    "            # Common Voice 9\n",
    "            if \"common_voice_9_0\" in dataset_name:\n",
    "                if input_str.startswith('\"') and input_str.endswith('\"'):\n",
    "                    # we can remove trailing quotation marks as they do not affect the transcription\n",
    "                    input_str = input_str[1:-1]\n",
    "                # replace double quotation marks with single\n",
    "                input_str = input_str.replace('\"\"', '\"')\n",
    "            # TED-LIUM (Release 3)\n",
    "            if \"tedlium\" in dataset_name:\n",
    "                # delete the <unk> token from the text\n",
    "                input_str = input_str.replace(\"<unk>\", \"\")\n",
    "                # replace spaced apostrophes with un-spaced (it 's -> it's)\n",
    "                for contraction in tedlium_contractions:\n",
    "                    input_str = input_str.replace(contraction, contraction[1:])\n",
    "            # GigaSpeech\n",
    "            if \"gigaspeech\" in dataset_name:\n",
    "                for disfluency in gigaspeech_disfluencies:\n",
    "                    input_str = input_str.replace(disfluency, \"\")\n",
    "                # convert spelled out punctuation to symbolic form\n",
    "                for punctuation, replacement in gigaspeech_punctuation.items():\n",
    "                    input_str = input_str.replace(punctuation, replacement)\n",
    "            # SWB: hide the path to the private HF dataset\n",
    "            if \"switchboard\" in dataset_name:\n",
    "                for disfluency in swb_disfluencies:\n",
    "                    input_str = input_str.replace(disfluency, \"\")\n",
    "                # remove parenthesised text (test data only)\n",
    "                input_str = re.sub(\"[\\(].*?[\\)]\", \"\", input_str)\n",
    "                for punctuation in swb_punctuations:\n",
    "                    input_str = input_str.replace(punctuation, \"\")\n",
    "                # replace anomalous words with their correct transcriptions\n",
    "                split_str = input_str.split(\"/\")\n",
    "                if len(split_str) > 1:\n",
    "                    input_str = \" \".join(\n",
    "                        [\" \".join([\" \".join(i.split(\" \")[:-1]) for i in split_str])] + [split_str[-1].split(\" \")[-1]])\n",
    "            # Earnings 22: still figuring out best segmenting method. Thus, dataset name subject to change\n",
    "            if \"earnings22\" in dataset_name:\n",
    "                for disfluency in earnings_disfluencies:\n",
    "                    input_str = input_str.replace(disfluency, \"\")\n",
    "            # SPGISpeech\n",
    "            if \"spgispeech\" in dataset_name:\n",
    "                pass  # no error correction necessary\n",
    "            # JIWER compliance (for WER/CER calc.)\n",
    "            # remove multiple spaces\n",
    "            input_str = re.sub(r\"\\s\\s+\", \" \", input_str)\n",
    "            # strip trailing spaces\n",
    "            input_str = input_str.strip()\n",
    "            batch[\"text\"] = input_str\n",
    "            batch[\"words_length\"] = len(input_str.split(\" \"))\n",
    "            return batch\n",
    "\n",
    "        ds = ds.map(prepare_dataset, desc=f\"pre-processing...\", num_proc=1)\n",
    "\n",
    "        def is_audio_not_none(audio_length):\n",
    "            return audio_length > 1\n",
    "\n",
    "        ds = ds.filter(is_audio_not_none, input_columns=[\"input_length\"], desc=\"filtering audio...\")\n",
    "\n",
    "        def is_text_not_none(words_length):\n",
    "            return words_length > 0\n",
    "\n",
    "        ds = ds.filter(is_text_not_none, input_columns=[\"words_length\"], desc=\"filtering text...\")\n",
    "\n",
    "        datasets[i] = ds\n",
    "\n",
    "        print(100*\"=\")\n",
    "        print(dataset_name)\n",
    "        print(\"Num samples: \", len(ds))\n",
    "        print(\"Total audio length: \", np.sum(ds[\"input_length\"]) / 60 ** 2, \"hours\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dad5c4-e631-4f7f-897e-44c467678c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = error_correction(train_datasets, ds_name, transcript_column_names, id_column_names, do_lower_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4286e-08bf-4210-a8ea-89b3c0d7ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_dev_clean = load_dataset(\"librispeech_asr\", \"all\", split=\"validation.clean[:10]\", use_auth_token=True)\n",
    "librispeech_dev_other = load_dataset(\"librispeech_asr\", \"all\", split=\"validation.other[:10]\", use_auth_token=True)\n",
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"all\", split=\"test.clean[:10]\", use_auth_token=True)\n",
    "librispeech_test_other = load_dataset(\"librispeech_asr\", \"all\", split=\"test.other[:10]\", use_auth_token=True)\n",
    "\n",
    "common_voice_9_0_dev = load_dataset(\"mozilla-foundation/common_voice_9_0\", \"en\", split=\"validation[:10]\", use_auth_token=True)\n",
    "common_voice_9_0_test = load_dataset(\"mozilla-foundation/common_voice_9_0\", \"en\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "voxpopuli_dev = load_dataset(\"google/xtreme_s\", \"voxpopuli.en\", split=\"validation[:10]\", use_auth_token=True)\n",
    "voxpopuli_test = load_dataset(\"google/xtreme_s\", \"voxpopuli.en\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "tedlium_dev = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"validation[:10]\", use_auth_token=True)\n",
    "tedlium_test = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "gigaspeech_dev = load_dataset(\"speechcolab/gigaspeech\", \"l\", split=\"validation[:10]\", use_auth_token=True)\n",
    "gigaspeech_test = load_dataset(\"speechcolab/gigaspeech\", \"l\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "earnings22_dev = load_dataset(\"sanchit-gandhi/earnings22_robust_split\", split=\"validation[:10]\", use_auth_token=True)\n",
    "earnings22_test = load_dataset(\"sanchit-gandhi/earnings22_robust_split\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "spgispeech_dev = load_dataset(\"kensho/spgispeech\", \"L\", split=\"validation\", use_auth_token=True, revision=\"f4d7d3b3f9b66414a09532ec937e285197afeaf6\")\n",
    "spgispeech_test = load_dataset(\"kensho/spgispeech\", \"L\", split=\"test\", use_auth_token=True, revision=\"f4d7d3b3f9b66414a09532ec937e285197afeaf6\")\n",
    "\n",
    "switchboard_test = load_dataset(\"ldc/switchboard\", \"switchboard\", split=\"test.switchboard[:10]\", use_auth_token=True)\n",
    "callhome_test = load_dataset(\"ldc/switchboard\", \"switchboard\", split=\"test.callhome[:10]\", use_auth_token=True)\n",
    "\n",
    "dev_ds = [librispeech_dev_clean, librispeech_dev_other, common_voice_9_0_dev, voxpopuli_dev, tedlium_dev, gigaspeech_dev, earnings22_dev, spgispeech_dev, switchboard_test]\n",
    "dev_name = [\"librispeech_asr/validation.clean\", \"librispeech_asr/validation.other\", \"common_voice_9_0/validation\", \"voxpopuli/validation\", \"tedlium/validation\", \"gigaspeech/validation\", \"earnings22/validation\", \"spgispeech/validation\", \"switchboard/test\"]\n",
    "\n",
    "test_ds = [librispeech_test_clean, librispeech_test_other, common_voice_9_0_test, voxpopuli_test, tedlium_test, gigaspeech_test, earnings22_test, spgispeech_test, callhome_test]\n",
    "test_name = [\"librispeech_asr/test.clean\", \"librispeech_asr/test.other\", \"common_voice_9_0/test\", \"voxpopuli/test\", \"tedlium/test\", \"gigaspeech/test\", \"earnings22/test\", \"spgispeech/test\", \"switchboard/callhome\"]\n",
    "\n",
    "dev_transcript_column_names = [transcript_column_names[0], *transcript_column_names]\n",
    "dev_id_column_names = [id_column_names[0], *id_column_names]\n",
    "dev_do_lower_cases = [do_lower_cases[0], *do_lower_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f03199-9060-4213-a8c5-834d79931a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ds = error_correction(dev_ds, dev_name, dev_transcript_column_names, dev_id_column_names, dev_do_lower_cases)\n",
    "test_ds = error_correction(test_ds, test_name, dev_transcript_column_names, dev_id_column_names, dev_do_lower_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d76bd9-8615-45b2-9ee2-33f1860beac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets for accumulated statistics (train-dev-test)\n",
    "librispeech_all = concatenate_datasets([train_datasets[0], dev_ds[0], dev_ds[1], test_ds[0], test_ds[1]])\n",
    "all_datasets = [concatenate_datasets([train_datasets[i-1], dev_ds[i], test_ds[i]]) for i in range(2, len(dev_name))]\n",
    "all_datasets = [librispeech_all, * all_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113a589-d103-41e3-a092-23d309fcfa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_datasets)):\n",
    "    print(ds_name[i], all_datasets[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b158a-7e7a-4529-80e5-0085726b0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_datasets)):\n",
    "    print(ds_name[i])\n",
    "    ds = all_datasets[i]\n",
    "    print(\"Mean sample duration: \", np.mean(ds[\"input_length\"]), \"+-\", np.std(ds[\"input_length\"]), \"s\")\n",
    "    print(\"Mean transcript length: \", np.mean(ds[\"words_length\"]), \"+-\", np.std(ds[\"words_length\"]), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d263e6-de02-475f-a11f-55e9208b7116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
