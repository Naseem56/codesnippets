{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10252c2b-3522-46de-b339-3a90038ef969",
   "metadata": {},
   "source": [
    "## Analysis of ERSR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fb02de-fcbf-41c8-9f8c-dac3f882baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit_huggingface_co/miniconda3/envs/ds/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import datasets\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e3775-31e2-46fe-b832-84efabbdfdd0",
   "metadata": {},
   "source": [
    "### Load all training datasets\n",
    "\n",
    "Note: for debugging/prototyping, only the first 10 samples of each datasets are currently loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a99f9d-972a-47c2-ab4f-721722cd0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset common_voice_9_0 (/home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d)\n",
      "Reusing dataset xtreme_s (/home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2)\n",
      "Reusing dataset tedlium (/home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d)\n",
      "Reusing dataset gigaspeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c)\n",
      "Using custom data configuration sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0\n",
      "Reusing dataset parquet (/home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Reusing dataset spgispeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0)\n",
      "Reusing dataset switchboard (/home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc)\n"
     ]
    }
   ],
   "source": [
    "# load all LS (clean, other) as one\n",
    "librispeech = load_dataset(\"librispeech_asr\", \"all\", split=\"train.clean.100[:10]+train.clean.360[:10]+train.other.500[:10]\", use_auth_token=True)\n",
    "\n",
    "common_voice_9_0 = load_dataset(\"mozilla-foundation/common_voice_9_0\", \"en\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "# load vox from xtreme-s split\n",
    "voxpopuli = load_dataset(\"google/xtreme_s\", \"voxpopuli.en\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "tedlium = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"l\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "earnings22 = load_dataset(\"sanchit-gandhi/earnings22_robust_split\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "# pin revision of kensho to avoid re-processing dataset after new modifications to spgispeech.py\n",
    "spgispeech = load_dataset(\"kensho/spgispeech\", \"L\", split=\"train[:10]\", use_auth_token=True, revision=\"f4d7d3b3f9b66414a09532ec937e285197afeaf6\")\n",
    "\n",
    "switchboard = load_dataset(\"ldc/switchboard\", \"switchboard\", split=\"train[:10]\", use_auth_token=True)\n",
    "\n",
    "train_datasets = [librispeech, common_voice_9_0, voxpopuli, tedlium, gigaspeech, earnings22, spgispeech, switchboard]\n",
    "ds_name = [\"librispeech\", \"common_voice_9_0\", \"voxpopuli\", \"tedlium\", \"gigaspeech\", \"earnings22\", \"spgispeech\", \"switchboard\"]\n",
    "\n",
    "# define text/id column names for each dataset\n",
    "transcript_column_names = ['text', 'sentence', 'transcription', 'text', 'text', 'sentence', 'transcript', 'text']\n",
    "id_column_names = ['id', 'client_id', 'id', 'id', 'segment_id', 'source_id', 'wav_filename', 'id']\n",
    "# whether to lower case each dataset\n",
    "do_lower_cases = [True, False, True, True, True, False, False, True]\n",
    "\n",
    "\n",
    "# define our 'error corrections' labels\n",
    "tedlium_contractions = [\" 's\", \" 't\", \" 're\", \" 've\", \" 'm\", \" 'll\", \" 'd\", \" 'clock\", \" 'all\"]\n",
    "gigaspeech_punctuation = {\" <comma>\": \",\", \" <period>\": \".\", \" <questionmark>\": \"?\", \" <exclamationpoint>\": \"!\"}\n",
    "gigaspeech_disfluencies = [\"<other>\", \"<sil>\"]\n",
    "swb_disfluencies = [\"[noise]\", \"[laughter]\", \"[silence]\", \"<a_aside>\", \"<b_aside>\", \"<e_aside>\", \"[laughter-\",\n",
    "                    \"[vocalized-noise]\", \"_1\"]\n",
    "swb_punctuations = [\"{\", \"}\", \"[\", \"]-\", \"]\"]\n",
    "earnings_disfluencies = [\"<crosstalk>\", \"<affirmative>\", \"<inaudible>\", \"inaudible\", \"<laugh>\"]\n",
    "ignore_segments = [\"ignore_time_segment_in_scoring\", \"<noise>\", \"<music>\", \"[noise]\", \"[laughter]\", \"[silence]\",\n",
    "                   \"[vocalized-noise]\", \"<crosstalk>\", \"<affirmative>\", \"<inaudible>\", \"<laugh>\", \"<other>\",\n",
    "                   \"<sil>\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cccd74ae-9466-4c59-843d-6212ab924d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_correction(datasets, ds_name, transcript_column_names, id_column_names, do_lower_cases):\n",
    "    \"\"\"\n",
    "    Function to:\n",
    "    - Rename text column name to a uniform name `text` (useful for subsequent analyses)\n",
    "    - Rename id column name to a uniform name `id`\n",
    "    - Filter samples not included for training/eval (those with 'junk' tokens only)\n",
    "    - ERSR error correction on a dataset-by-dataset basis\n",
    "    - Filter samples by a zero audio/text length criterion\n",
    "    - Print num samples and total hours for each dataset\n",
    "    \n",
    "    Returns:\n",
    "    - Error corrected list of datasets\n",
    "    \"\"\"\n",
    "    for i, ds in enumerate(datasets):\n",
    "        dataset_name = ds_name[i]\n",
    "        text_column_name = transcript_column_names[i]\n",
    "        id_column_name = id_column_names[i]\n",
    "        do_lower_case = do_lower_cases[i]\n",
    "\n",
    "        if text_column_name != \"text\":\n",
    "            ds = ds.rename_column(text_column_name, \"text\")\n",
    "        if id_column_name != \"id\":\n",
    "            ds = ds.rename_column(id_column_name, \"id\")\n",
    "\n",
    "        def is_target_labels(input_str):\n",
    "            return input_str.lower() not in ignore_segments\n",
    "\n",
    "        ds = ds.filter(is_target_labels, input_columns=[\"text\"], desc=\"filtering text...\")\n",
    "\n",
    "        def prepare_dataset(batch):\n",
    "            \"\"\"Entirely follows ERSR error correction\"\"\"\n",
    "            # Pre-process audio\n",
    "            try:\n",
    "                sample = batch[\"audio\"]\n",
    "            except ValueError:\n",
    "                # E22: some samples are empty (no audio). Reading the empty audio array will trigger\n",
    "                # a soundfile ValueError. For now, we'll manually set these arrays to a zero array.\n",
    "                # They will be filtered in the subsequent filtering stage and so are\n",
    "                # explicitly ignored during training.\n",
    "                sample = {\"array\": np.array([0.]), \"sampling_rate\": 16000}\n",
    "\n",
    "            # time in s\n",
    "            batch[\"input_length\"] = len(sample[\"array\"]) / sample[\"sampling_rate\"]\n",
    "\n",
    "            # 'Error correction' of targets\n",
    "            input_str = batch[\"text\"].lower() if do_lower_case else batch[\"text\"]\n",
    "            # LibriSpeech ASR\n",
    "            if \"librispeech\" in dataset_name:\n",
    "                pass  # no error correction necessary\n",
    "\n",
    "            # VoxPopuli\n",
    "            if \"voxpopuli\" in dataset_name:\n",
    "                pass  # no error correction necessary\n",
    "            \n",
    "            # Common Voice 9\n",
    "            if \"common_voice_9_0\" in dataset_name:\n",
    "                if input_str.startswith('\"') and input_str.endswith('\"'):\n",
    "                    # we can remove trailing quotation marks as they do not affect the transcription\n",
    "                    input_str = input_str[1:-1]\n",
    "                # replace double quotation marks with single\n",
    "                input_str = input_str.replace('\"\"', '\"')\n",
    "            \n",
    "            # TED-LIUM (Release 3)\n",
    "            if \"tedlium\" in dataset_name:\n",
    "                # delete the <unk> token from the text\n",
    "                input_str = input_str.replace(\"<unk>\", \"\")\n",
    "                # replace spaced apostrophes with un-spaced (it 's -> it's)\n",
    "                for contraction in tedlium_contractions:\n",
    "                    input_str = input_str.replace(contraction, contraction[1:])\n",
    "            \n",
    "            # GigaSpeech\n",
    "            if \"gigaspeech\" in dataset_name:\n",
    "                for disfluency in gigaspeech_disfluencies:\n",
    "                    input_str = input_str.replace(disfluency, \"\")\n",
    "                # convert spelled out punctuation to symbolic form\n",
    "                for punctuation, replacement in gigaspeech_punctuation.items():\n",
    "                    input_str = input_str.replace(punctuation, replacement)\n",
    "            \n",
    "            # SWB: hide the path to the private HF dataset\n",
    "            if \"switchboard\" in dataset_name:\n",
    "                for disfluency in swb_disfluencies:\n",
    "                    input_str = input_str.replace(disfluency, \"\")\n",
    "                # remove parenthesised text (test data only)\n",
    "                input_str = re.sub(\"[\\(].*?[\\)]\", \"\", input_str)\n",
    "                for punctuation in swb_punctuations:\n",
    "                    input_str = input_str.replace(punctuation, \"\")\n",
    "                # replace anomalous words with their correct transcriptions\n",
    "                split_str = input_str.split(\"/\")\n",
    "                if len(split_str) > 1:\n",
    "                    input_str = \" \".join(\n",
    "                        [\" \".join([\" \".join(i.split(\" \")[:-1]) for i in split_str])] + [split_str[-1].split(\" \")[-1]])\n",
    "            \n",
    "            # Earnings 22\n",
    "            if \"earnings22\" in dataset_name:\n",
    "                for disfluency in earnings_disfluencies:\n",
    "                    input_str = input_str.replace(disfluency, \"\")\n",
    "            \n",
    "            # SPGISpeech\n",
    "            if \"spgispeech\" in dataset_name:\n",
    "                pass  # no error correction necessary\n",
    "            \n",
    "            # JIWER compliance (for WER/CER calc.)\n",
    "            # remove multiple spaces\n",
    "            input_str = re.sub(r\"\\s\\s+\", \" \", input_str)\n",
    "            # strip trailing spaces\n",
    "            input_str = input_str.strip()\n",
    "\n",
    "            batch[\"text\"] = input_str\n",
    "            batch[\"text_length\"] = len(input_str.split(\" \"))\n",
    "            return batch\n",
    "\n",
    "        ds = ds.map(prepare_dataset, desc=f\"pre-processing...\", num_proc=1)\n",
    "\n",
    "        def is_audio_empty(audio_length):\n",
    "            # remove empty Earnigns22 audio samples (length = 1)\n",
    "            return audio_length > 1\n",
    "\n",
    "        ds = ds.filter(is_audio_empty, input_columns=[\"input_length\"], desc=\"filtering audio...\")\n",
    "\n",
    "        def is_text_empty(words_length):\n",
    "            return words_length > 0\n",
    "\n",
    "        ds = ds.filter(is_text_empty, input_columns=[\"text_length\"], desc=\"filtering text...\")\n",
    "\n",
    "        datasets[i] = ds\n",
    "\n",
    "        print(100*\"=\")\n",
    "        print(dataset_name)\n",
    "        print(\"Num samples: \", len(ds))\n",
    "        print(\"Total audio length: \", np.sum(ds[\"input_length\"]) / 60 ** 2, \"hours\")\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48dad5c4-e631-4f7f-897e-44c467678c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-cec0e1532fb7dc2b.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-0dd3ed07ad374ed7.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-058ac256820953a0.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-1b44a2f13f251523.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-99194521a8fa9dd8.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-fa7e5cadab6f2976.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-0688043ae2578dcb.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-b37dbcd0d6eb3ab6.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-30a3e8518150a3b6.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-d9df22034f2855c8.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-1e15463ad4eeb2b6.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-01c913e21fa44778.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-cd7d7eec5e147da6.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-2181b860d31f4e02.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-90c3ad359101ced2.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-df88f3b2a3b95cc0.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-01b4f1d079f11156.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-696364519ff054a4.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-9cb301237b4903f3.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-30a22ad9a38bb654.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-61b1efe30539c089.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9a56d5429f5574f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "librispeech\n",
      "Num samples:  30\n",
      "Total audio length:  0.10402222222222222 hours\n",
      "====================================================================================================\n",
      "common_voice_9_0\n",
      "Num samples:  10\n",
      "Total audio length:  0.01198 hours\n",
      "====================================================================================================\n",
      "voxpopuli\n",
      "Num samples:  10\n",
      "Total audio length:  0.04011559027777778 hours\n",
      "====================================================================================================\n",
      "tedlium\n",
      "Num samples:  10\n",
      "Total audio length:  0.018216666666666666 hours\n",
      "====================================================================================================\n",
      "gigaspeech\n",
      "Num samples:  10\n",
      "Total audio length:  0.009133333333333334 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8b1bf630ed50e578.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a9ffbc0d72f3ac12.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0/cache-b2ba4bceae4b3055.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0/cache-a4fb1c2d2a7533c2.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0/cache-02421b860d628456.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0/cache-e4bbe23dbc366e63.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc/cache-bce159a204b9546b.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc/cache-c7ab75cb7b200d29.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc/cache-2efa28791522ea21.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc/cache-f80c225dbcb5f894.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "earnings22\n",
      "Num samples:  9\n",
      "Total audio length:  0.016061163194444446 hours\n",
      "====================================================================================================\n",
      "spgispeech\n",
      "Num samples:  10\n",
      "Total audio length:  0.024291666666666663 hours\n",
      "====================================================================================================\n",
      "switchboard\n",
      "Num samples:  4\n",
      "Total audio length:  0.005201180555555555 hours\n"
     ]
    }
   ],
   "source": [
    "train_datasets = error_correction(train_datasets, ds_name, transcript_column_names, id_column_names, do_lower_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dbc8b-dd6a-4d7f-ba0f-7ca64d4aaab3",
   "metadata": {},
   "source": [
    "### Load all dev/test sets\n",
    "\n",
    "We do this on a dev/test split-by-split basis to get the size (hours) of each split, info we require for the ERSR paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a4286e-08bf-4210-a8ea-89b3c0d7ea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset librispeech_asr (/home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset librispeech_asr (/home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset librispeech_asr (/home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n",
      "Reusing dataset common_voice_9_0 (/home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d)\n",
      "Reusing dataset common_voice_9_0 (/home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d)\n",
      "Reusing dataset xtreme_s (/home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2)\n",
      "Reusing dataset xtreme_s (/home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2)\n",
      "Reusing dataset tedlium (/home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d)\n",
      "Reusing dataset tedlium (/home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d)\n",
      "Reusing dataset gigaspeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c)\n",
      "Reusing dataset gigaspeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c)\n",
      "Using custom data configuration sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0\n",
      "Reusing dataset parquet (/home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0\n",
      "Reusing dataset parquet (/home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Reusing dataset spgispeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0)\n",
      "Reusing dataset spgispeech (/home/sanchit_huggingface_co/.cache/huggingface/datasets/kensho___spgispeech/L/1.0.0/9c55755e8cc1d73e7c24cd76053daa3737ca6d7b42c04fde14d026bd0dc12de0)\n",
      "Reusing dataset switchboard (/home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc)\n",
      "Reusing dataset switchboard (/home/sanchit_huggingface_co/.cache/huggingface/datasets/ldc___switchboard/switchboard/1.1.0/e83ceff84c9bde86ee867eba797ce560151517683a7efd276521181551e305cc)\n"
     ]
    }
   ],
   "source": [
    "librispeech_dev_clean = load_dataset(\"librispeech_asr\", \"all\", split=\"validation.clean[:10]\", use_auth_token=True)\n",
    "librispeech_dev_other = load_dataset(\"librispeech_asr\", \"all\", split=\"validation.other[:10]\", use_auth_token=True)\n",
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"all\", split=\"test.clean[:10]\", use_auth_token=True)\n",
    "librispeech_test_other = load_dataset(\"librispeech_asr\", \"all\", split=\"test.other[:10]\", use_auth_token=True)\n",
    "\n",
    "common_voice_9_0_dev = load_dataset(\"mozilla-foundation/common_voice_9_0\", \"en\", split=\"validation[:10]\", use_auth_token=True)\n",
    "common_voice_9_0_test = load_dataset(\"mozilla-foundation/common_voice_9_0\", \"en\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "voxpopuli_dev = load_dataset(\"google/xtreme_s\", \"voxpopuli.en\", split=\"validation[:10]\", use_auth_token=True)\n",
    "voxpopuli_test = load_dataset(\"google/xtreme_s\", \"voxpopuli.en\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "tedlium_dev = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"validation[:10]\", use_auth_token=True)\n",
    "tedlium_test = load_dataset(\"LIUM/tedlium\", \"release3\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "gigaspeech_dev = load_dataset(\"speechcolab/gigaspeech\", \"l\", split=\"validation[:10]\", use_auth_token=True)\n",
    "gigaspeech_test = load_dataset(\"speechcolab/gigaspeech\", \"l\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "earnings22_dev = load_dataset(\"sanchit-gandhi/earnings22_robust_split\", split=\"validation[:10]\", use_auth_token=True)\n",
    "earnings22_test = load_dataset(\"sanchit-gandhi/earnings22_robust_split\", split=\"test[:10]\", use_auth_token=True)\n",
    "\n",
    "spgispeech_dev = load_dataset(\"kensho/spgispeech\", \"L\", split=\"validation[:10]\", use_auth_token=True, revision=\"f4d7d3b3f9b66414a09532ec937e285197afeaf6\")\n",
    "spgispeech_test = load_dataset(\"kensho/spgispeech\", \"L\", split=\"test[:10]\", use_auth_token=True, revision=\"f4d7d3b3f9b66414a09532ec937e285197afeaf6\")\n",
    "\n",
    "switchboard_test = load_dataset(\"ldc/switchboard\", \"switchboard\", split=\"test.switchboard[:10]\", use_auth_token=True)\n",
    "callhome_test = load_dataset(\"ldc/switchboard\", \"switchboard\", split=\"test.callhome[:10]\", use_auth_token=True)\n",
    "\n",
    "dev_ds = [librispeech_dev_clean, librispeech_dev_other, common_voice_9_0_dev, voxpopuli_dev, tedlium_dev, gigaspeech_dev, earnings22_dev, spgispeech_dev, switchboard_test]\n",
    "dev_name = [\"librispeech_asr/validation.clean\", \"librispeech_asr/validation.other\", \"common_voice_9_0/validation\", \"voxpopuli/validation\", \"tedlium/validation\", \"gigaspeech/validation\", \"earnings22/validation\", \"spgispeech/validation\", \"switchboard/test\"]\n",
    "\n",
    "test_ds = [librispeech_test_clean, librispeech_test_other, common_voice_9_0_test, voxpopuli_test, tedlium_test, gigaspeech_test, earnings22_test, spgispeech_test, callhome_test]\n",
    "test_name = [\"librispeech_asr/test.clean\", \"librispeech_asr/test.other\", \"common_voice_9_0/test\", \"voxpopuli/test\", \"tedlium/test\", \"gigaspeech/test\", \"earnings22/test\", \"spgispeech/test\", \"switchboard/callhome\"]\n",
    "\n",
    "# we now have two LS dev & test sets (clean/other) -> update transcript/id column names accordingly by repeating the first entry\n",
    "dev_transcript_column_names = [transcript_column_names[0], *transcript_column_names]\n",
    "dev_id_column_names = [id_column_names[0], *id_column_names]\n",
    "dev_do_lower_cases = [do_lower_cases[0], *do_lower_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f03199-9060-4213-a8c5-834d79931a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-a0c14a2469e17a91.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-b5044f8164ec061b.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-a2f6ad785a823753.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-187e4e41d70d6342.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-8d371e074a0f5d1a.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-26e0401152c183ed.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-f0069f1716040d86.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2a9be1c08186c444.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-fe01e8a2643f8858.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-aa723e5be1746b66.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-e8e1e3fec7e88806.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/mozilla-foundation___common_voice_9_0/en/9.0.0/c8491634a4579fef5745ab949ee9aa4265b7203d7e2ecf44f45879a6419cd40d/cache-ee6823b99f625e3c.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-de13f72f0a896e3d.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-ac153d3557341ed3.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-4de586c3e4bfb1de.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/google___xtreme_s/voxpopuli.en/2.0.0/1384f19b49cc1beade2a9bf2ca44abe870cd95f85819a16f6f44671d4fdad7e2/cache-75f91401ff04b857.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-e408bc8135f0943b.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-6fb2b89238a517b5.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-f5819f7c4c246ed1.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/LIUM___tedlium/release3/1.0.1/3534cf671f9fe252aa91994765f9fbe95f9a077a67d56255dcd6645776ab997d/cache-a3f01c0112d6962b.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-90f98ae8727814b5.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-6cca5d63a6034b4d.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-571e8932be43db8e.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/speechcolab___gigaspeech/l/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/cache-065964a34a46e50b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "librispeech_asr/validation.clean\n",
      "Num samples:  10\n",
      "Total audio length:  0.0144625 hours\n",
      "====================================================================================================\n",
      "librispeech_asr/validation.other\n",
      "Num samples:  10\n",
      "Total audio length:  0.013333333333333336 hours\n",
      "====================================================================================================\n",
      "common_voice_9_0/validation\n",
      "Num samples:  10\n",
      "Total audio length:  0.012313333333333334 hours\n",
      "====================================================================================================\n",
      "voxpopuli/validation\n",
      "Num samples:  10\n",
      "Total audio length:  0.023910972222222222 hours\n",
      "====================================================================================================\n",
      "tedlium/validation\n",
      "Num samples:  7\n",
      "Total audio length:  0.022930555555555558 hours\n",
      "====================================================================================================\n",
      "gigaspeech/validation\n",
      "Num samples:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8cbe3291d5aaabd2.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0f2e3014e0ad72b0.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cb7bb1437e99ac41.arrow\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--earnings22_robust_split-0404c6cc081bf5f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f8bbbdd63cca4287.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio length:  0.01107138888888889 hours\n",
      "====================================================================================================\n",
      "earnings22/validation\n",
      "Num samples:  9\n",
      "Total audio length:  0.019405711805555555 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.76ba/s]\n",
      "pre-processing...: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.27s/ex]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 452.36ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 409.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "spgispeech/validation\n",
      "Num samples:  10\n",
      "Total audio length:  0.02225833333333333 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.42ba/s]\n",
      "pre-processing...: 100%|███████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 726.53ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 408.80ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 548.63ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "switchboard/test\n",
      "Num samples:  9\n",
      "Total audio length:  0.010027777777777776 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 517.05ba/s]\n",
      "pre-processing...: 100%|███████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 237.31ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 463.15ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 390.57ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "librispeech_asr/test.clean\n",
      "Num samples:  10\n",
      "Total audio length:  0.025423611111111112 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 423.75ba/s]\n",
      "pre-processing...: 100%|███████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 337.09ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 466.76ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 429.30ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "librispeech_asr/test.other\n",
      "Num samples:  10\n",
      "Total audio length:  0.014059722222222223 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 279.17ba/s]\n",
      "pre-processing...: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 77.28ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 490.56ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 395.95ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "common_voice_9_0/test\n",
      "Num samples:  10\n",
      "Total audio length:  0.015833333333333335 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 279.19ba/s]\n",
      "pre-processing...: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.30ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 512.75ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 416.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "voxpopuli/test\n",
      "Num samples:  10\n",
      "Total audio length:  0.031192447916666664 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 283.25ba/s]\n",
      "pre-processing...: 100%|█████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 257.11ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 468.74ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 404.43ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "tedlium/test\n",
      "Num samples:  6\n",
      "Total audio length:  0.015338402777777779 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 442.62ba/s]\n",
      "pre-processing...: 100%|██████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.64ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 545.85ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 290.93ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "gigaspeech/test\n",
      "Num samples:  8\n",
      "Total audio length:  0.02004027777777778 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 286.99ba/s]\n",
      "pre-processing...:   0%|                                                                         | 0/10 [00:00<?, ?ex/s]/home/sanchit_huggingface_co/miniconda3/envs/ds/lib/python3.9/site-packages/datasets/features/audio.py:278: FutureWarning: Pass orig_sr=44100, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  array = librosa.resample(array, sampling_rate, self.sampling_rate, res_type=\"kaiser_best\")\n",
      "/home/sanchit_huggingface_co/miniconda3/envs/ds/lib/python3.9/site-packages/datasets/features/audio.py:278: FutureWarning: Pass orig_sr=24000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  array = librosa.resample(array, sampling_rate, self.sampling_rate, res_type=\"kaiser_best\")\n",
      "/home/sanchit_huggingface_co/miniconda3/envs/ds/lib/python3.9/site-packages/datasets/features/audio.py:278: FutureWarning: Pass orig_sr=32000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  array = librosa.resample(array, sampling_rate, self.sampling_rate, res_type=\"kaiser_best\")\n",
      "pre-processing...:  60%|███████████████████████████████████████                          | 6/10 [00:00<00:00, 42.23ex/s]/home/sanchit_huggingface_co/miniconda3/envs/ds/lib/python3.9/site-packages/datasets/features/audio.py:278: FutureWarning: Pass orig_sr=48000, target_sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  array = librosa.resample(array, sampling_rate, self.sampling_rate, res_type=\"kaiser_best\")\n",
      "pre-processing...: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 42.36ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 606.90ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 385.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "earnings22/test\n",
      "Num samples:  8\n",
      "Total audio length:  0.016889114583333333 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 508.22ba/s]\n",
      "pre-processing...: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.68ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 478.15ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "spgispeech/test\n",
      "Num samples:  10\n",
      "Total audio length:  0.021283333333333335 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 239.92ba/s]\n",
      "pre-processing...: 100%|███████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 901.79ex/s]\n",
      "filtering audio...: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 539.04ba/s]\n",
      "filtering text...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 452.70ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "switchboard/callhome\n",
      "Num samples:  8\n",
      "Total audio length:  0.006383333333333334 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# error correction of dev/test sets\n",
    "dev_ds = error_correction(dev_ds, dev_name, dev_transcript_column_names, dev_id_column_names, dev_do_lower_cases)\n",
    "test_ds = error_correction(test_ds, test_name, dev_transcript_column_names, dev_id_column_names, dev_do_lower_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2d179-6ef2-4387-a87a-3b763f37d157",
   "metadata": {},
   "source": [
    "### Combine datasets for accumulated statistics (train-dev-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d76bd9-8615-45b2-9ee2-33f1860beac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS has two dev/test sets (clean/other) -> treat separately\n",
    "librispeech_all = concatenate_datasets([train_datasets[0], dev_ds[0], dev_ds[1], test_ds[0], test_ds[1]])\n",
    "# rule based approach for combining remaineder of datasets: combine train with dev and test\n",
    "all_datasets = [concatenate_datasets([train_datasets[i-1], dev_ds[i], test_ds[i]]) for i in range(2, len(dev_name))]\n",
    "# append LS\n",
    "all_datasets = [librispeech_all, * all_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6113a589-d103-41e3-a092-23d309fcfa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librispeech Dataset({\n",
      "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'input_length', 'text_length'],\n",
      "    num_rows: 70\n",
      "})\n",
      "common_voice_9_0 Dataset({\n",
      "    features: ['id', 'path', 'audio', 'text', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'input_length', 'text_length'],\n",
      "    num_rows: 30\n",
      "})\n",
      "voxpopuli Dataset({\n",
      "    features: ['id', 'path', 'audio', 'text', 'lang_id', 'input_length', 'text_length'],\n",
      "    num_rows: 30\n",
      "})\n",
      "tedlium Dataset({\n",
      "    features: ['audio', 'text', 'speaker_id', 'gender', 'file', 'id', 'input_length', 'text_length'],\n",
      "    num_rows: 23\n",
      "})\n",
      "gigaspeech Dataset({\n",
      "    features: ['id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path', 'input_length', 'text_length'],\n",
      "    num_rows: 26\n",
      "})\n",
      "earnings22 Dataset({\n",
      "    features: ['id', 'audio', 'segment_id', 'text', 'start_ts', 'end_ts', 'input_length', 'text_length'],\n",
      "    num_rows: 26\n",
      "})\n",
      "spgispeech Dataset({\n",
      "    features: ['id', 'audio', 'wav_filesize', 'text', 'input_length', 'text_length'],\n",
      "    num_rows: 30\n",
      "})\n",
      "switchboard Dataset({\n",
      "    features: ['audio', 'text', 'file', 'id', 'input_length', 'text_length'],\n",
      "    num_rows: 21\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_datasets)):\n",
    "    print(ds_name[i], all_datasets[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491188c-a2ed-4c02-bf3a-4ee68f975eda",
   "metadata": {},
   "source": [
    "Great! Now that we've combined our train/dev/test splits, we can run a combined analysis over all splits. The column names have also been normalised `(audio, input_length, text, text_length)` for our convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4b158a-7e7a-4529-80e5-0085726b0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librispeech\n",
      "Mean sample duration: 8.809785714285715 +- 5.019140111023166 s\n",
      "Mean transcript length: 24.97142857142857 +- 13.094133286728535 words\n",
      "common_voice_9_0\n",
      "Mean sample duration: 4.8152 +- 2.021359384176896 s\n",
      "Mean transcript length: 8.566666666666666 +- 4.047083998249714 words\n",
      "voxpopuli\n",
      "Mean sample duration: 11.426281249999999 +- 5.961289670369112 s\n",
      "Mean transcript length: 29.333333333333332 +- 12.242911781471307 words\n",
      "tedlium\n",
      "Mean sample duration: 8.841228260869565 +- 3.5927036589522867 s\n",
      "Mean transcript length: 22.782608695652176 +- 13.484150185984479 words\n",
      "gigaspeech\n",
      "Mean sample duration: 5.572384615384616 +- 3.4397609384388135 s\n",
      "Mean transcript length: 17.423076923076923 +- 13.255399703584013 words\n",
      "earnings22\n",
      "Mean sample duration: 7.249290865384617 +- 5.429852291244234 s\n",
      "Mean transcript length: 17.307692307692307 +- 14.488109795407645 words\n",
      "spgispeech\n",
      "Mean sample duration: 8.14 +- 2.532520483628909 s\n",
      "Mean transcript length: 20.1 +- 8.162311764036788 words\n",
      "switchboard\n",
      "Mean sample duration: 3.7049642857142855 +- 2.6675516332371894 s\n",
      "Mean transcript length: 11.476190476190476 +- 9.796107329999078 words\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_datasets)):\n",
    "    print(ds_name[i])\n",
    "    ds = all_datasets[i]\n",
    "    print(f\"Mean sample duration: {np.mean(ds['input_length'])} +- {np.std(ds['input_length'])} s\")\n",
    "    print(f\"Mean transcript length: {np.mean(ds['text_length'])} +- {np.std(ds['text_length'])} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d263e6-de02-475f-a11f-55e9208b7116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
